{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a bug in the Python interface; _source and _target do not work properly\n",
    "#on undirected graphs. As a workaround, use g.incident(x) to obtain the IDs of\n",
    "#the edges incident on vertex x -- this can then be used to subset g.es:\n",
    "    \n",
    "#    Using g.es[g.incident(idx)] solved\n",
    "\n",
    "##\n",
    "# bin format chr1:x1-x2\n",
    "# corresponding bed format chr1    x1    x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#import glob\n",
    "from pybedtools import BedTool\n",
    "\n",
    "from igraph import *\n",
    "#import matplotlib\n",
    "#matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#sns.set_style(\"white\") \n",
    "\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_loops_to_graph_bp(df_loops):\n",
    "    ## loop format: ['#chr1', 'x1', 'x2', 'chr2', 'y1', 'y2', 'GeneID']\n",
    "    df_bins = Loops_Return_two_bins_no_dup(df_loops)\n",
    "    df_bins['name'] = df_bins['#chr1'].astype(str)+':'+df_bins['x1'].astype(str)+'-'+df_bins['x2'].astype(str)\n",
    "    Num_vs = len(df_bins.index)\n",
    "    ## Initiation a graph from loops file \n",
    "\n",
    "    graph = Graph()\n",
    "    graph.add_vertices(Num_vs)\n",
    "    graph.vs[\"name\"] = df_bins['name']\n",
    "\n",
    "    df_edge = df_loops.merge(df_bins, on=['#chr1', 'x1', 'x2']).merge(\n",
    "        df_bins, left_on=['chr2', 'y1', 'y2'], right_on=['#chr1', 'x1', 'x2']).loc[:,['index_x','index_y']]\n",
    "\n",
    "    graph.add_edges(df_edge.values)\n",
    "    return graph\n",
    "\n",
    "def convert_loops_to_graph(df_loops, weight_col, _extra_edge_col):\n",
    "    ## loop format: ['#chr1', 'x1', 'x2', 'chr2', 'y1', 'y2', 'GeneID', 'weight_cols']\n",
    "    df_bins = Loops_Return_two_bins_no_dup(df_loops)\n",
    "    df_bins['name'] = df_bins['#chr1'].astype(str)+':'+df_bins['x1'].astype(str)+'-'+df_bins['x2'].astype(str)\n",
    "    Num_vs = len(df_bins.index)\n",
    "    ## Initiation a graph from loops file \n",
    "\n",
    "    graph = Graph()\n",
    "    graph.add_vertices(Num_vs)\n",
    "    graph.vs[\"name\"] = df_bins.loc[:,'name']\n",
    "    \n",
    "\n",
    "    df_edge = df_loops.merge(df_bins, on=['#chr1', 'x1', 'x2']).merge(\n",
    "        df_bins, left_on=['chr2', 'y1', 'y2'], right_on=['#chr1', 'x1', 'x2'])\n",
    "    graph.add_edges(df_edge.loc[:, ['index_x','index_y']].values)\n",
    "    if weight_col:\n",
    "        graph.es[\"weight\"] = df_edge.loc[:,weight_col].values\n",
    "    if _extra_edge_col:\n",
    "        graph.es[_extra_edge_col] = df_edge.loc[:,_extra_edge_col].values\n",
    "    return graph\n",
    "\n",
    "\n",
    "\n",
    "def Loops_Return_two_bins_no_dup(df_hic):\n",
    "    ## Associated by promoter\n",
    "    second_bin_columns = [3,4,5,0,1,2]+list(range(6,len(df_hic.columns),1))\n",
    "    df_hic=df_hic.append(pd.DataFrame(df_hic.iloc[:, second_bin_columns].values, columns=df_hic.columns),sort=False).sort_index()\n",
    "    return df_hic.iloc[:,0:3].drop_duplicates().reset_index().drop('index',axis=1).reset_index()\n",
    "\n",
    "def convert_cluster2bed(df_cluster, usecol):\n",
    "    df_tem = df_cluster[usecol].str.split(r\"\\:|-\",expand=True)\n",
    "    df_tem = pd.concat( [df_tem, df_cluster], axis=1)\n",
    "    if (df_tem.iloc[0,0].find('chr') == -1):\n",
    "        df_tem[0] = 'chr'+df_tem[0]\n",
    "    return df_tem\n",
    "\n",
    "def convert_bin2bed(df_cluster, col_name):\n",
    "    df_tem = df_cluster[col_name].str.split(r\"\\:|-\",expand=True)\n",
    "    df_tem = pd.concat( [df_tem, df_cluster], axis=1)\n",
    "    if (df_tem.iloc[0,0].find('chr') == -1):\n",
    "        df_tem[0] = 'chr'+df_tem[0]\n",
    "    return df_tem\n",
    "\n",
    "def convert_vs2bed(input_graph, col_name):\n",
    "    ## output first 3 columns is standard bed format\n",
    "    df_tem = pd.DataFrame(data={col_name:input_graph.vs[col_name]})\n",
    "    df_tem = pd.concat( [df_tem[col_name].str.split(r\"\\:|-\",expand=True),df_tem], axis=1)\n",
    "    if (df_tem.iloc[0,0].find('chr') == -1):\n",
    "        df_tem[0] = 'chr'+df_tem[0]\n",
    "    return df_tem\n",
    "\n",
    "def convert_graph_vs_to_df(_input_graph):\n",
    "    df_vs = pd.DataFrame(data= {\"degree\":_input_graph.degree()})\n",
    "    for col in _input_graph.vs.attributes():\n",
    "        df_vs[col] = _input_graph.vs[col]\n",
    "\n",
    "    return df_vs\n",
    "\n",
    "def graph_community_multilevel_Blondel(input_graph, cutoff):\n",
    "    ## input graph should have at least one attribute: name\n",
    "    df_vs = convert_graph_vs_to_df(input_graph)\n",
    "    _col_vs_name='name'\n",
    "    if (input_graph.is_weighted()):\n",
    "        print (\"Weighted Graph Cluster\")\n",
    "        structure = input_graph.community_multilevel(weights=input_graph.es['weight'] ,return_levels=False)\n",
    "    else:\n",
    "        structure = input_graph.community_multilevel(return_levels=False)\n",
    "    df_vs['membership'] = structure.membership\n",
    "    df_vs_cluster_group = df_vs.groupby('membership')\n",
    "    \n",
    "    ## Rank each cluster by number of bins\n",
    "    cluster_name=[]\n",
    "    cluster_num_vertices=[]\n",
    "    for df_vs_cluster in df_vs_cluster_group:\n",
    "        df_vs_inside_cluster = Cluster_Filter_by_Denisty(df_vs_cluster[1], _col_vs_name, 'degree', cutoff)\n",
    "        #df_vs_inside_cluster =df_vs_cluster[1]\n",
    "        df_cluster_coordiante = df_vs_inside_cluster[_col_vs_name].str.split(r\"\\:|-\",expand=True)\n",
    "        cluster_coordinate = 'chr'+df_cluster_coordiante.iloc[0,0]+':'+str(df_cluster_coordiante.iloc[:,1].astype(int).min())+'-'+str(df_cluster_coordiante.iloc[:,2].astype(int).max())\n",
    "        cluster_name.append(cluster_coordinate) ##0: cluster name\n",
    "        cluster_num_vertices.append(len(df_vs_inside_cluster)) # 1: num_vertices\n",
    "    \n",
    "    df_cluster_output = pd.DataFrame(data={'hub_name':cluster_name,'Num_vertices':cluster_num_vertices}).sort_values('Num_vertices', ascending=False)\n",
    "    return df_cluster_output, df_vs_cluster_group\n",
    "\n",
    "def Graph_Pagerank(_input_graph):\n",
    "    _input_graph.vs['pagerank'] = _input_graph.pagerank(weights=_input_graph.es['weight'])\n",
    "    return _input_graph\n",
    "\n",
    "def Cluster_Filter_by_Denisty(_df_vs_cluster, _col_name, _core_col, _cutoff):\n",
    "    ## Linear Denisty Threshold, 1 edge at least 1 anchor\n",
    "    cutoff=_cutoff#0.5\n",
    "    df_tem = _df_vs_cluster\n",
    "    col_name='name'\n",
    "    _core_col='pagerank'\n",
    "    resolution=10000\n",
    "    df_tem[col_name].str.split(r\"\\:|-\",expand=True)\n",
    "    df_tem = pd.concat( [df_tem[col_name].str.split(r\"\\:|-\",expand=True),df_tem], axis=1)\n",
    "\n",
    "    ## Define highest degree as summit\n",
    "    #num_core = int(len(df_tem)/3)+1\n",
    "    num_core = 1\n",
    "    core = df_tem.nlargest(int(num_core), _core_col).iloc[:,1].astype(int).mean()\n",
    "    #core = df_tem.nlargest(5, _core_col).iloc[:,1].mean()\n",
    "    df_tem['density'] = df_tem['degree'].astype(float)/(abs(df_tem.iloc[:,1].astype(float)-float(core))**2)*resolution**2\n",
    "\n",
    "    return df_tem[df_tem['density']>cutoff]\n",
    "\n",
    "\n",
    "\n",
    "def graph_community_multilevel_Blondel_diff_level(input_graph, cutoff):\n",
    "    ## input graph should have at least one attribute: name\n",
    "    df_vs = convert_graph_vs_to_df(input_graph)\n",
    "    _col_vs_name='name'\n",
    "    \n",
    "    if (input_graph.is_weighted()):\n",
    "        print (\"Weighted Graph Cluster\")\n",
    "        structure = input_graph.community_multilevel(weights=input_graph.es['weight'], return_levels=True)\n",
    "    else:\n",
    "        structure = input_graph.community_multilevel(return_levels=True)\n",
    "    \n",
    "    for tem_level in structure:\n",
    "        print (tem_level.summary())\n",
    "    df_vs['membership'] = structure[0].membership\n",
    "    df_vs_cluster_group = df_vs.groupby('membership')\n",
    "    \n",
    "    ## Rank each cluster by number of bins\n",
    "    cluster_name=[]\n",
    "    cluster_num_vertices=[]\n",
    "    for df_vs_cluster in df_vs_cluster_group:\n",
    "        df_vs_inside_cluster = Cluster_Filter_by_Denisty(df_vs_cluster[1], _col_vs_name, 'degree', cutoff)\n",
    "        if (len(df_vs_inside_cluster)>0):\n",
    "            df_cluster_coordiante = df_vs_inside_cluster[_col_vs_name].str.split(r\"\\:|-\",expand=True)\n",
    "            #print (df_cluster_coordiante)\n",
    "            cluster_coordinate = 'chr'+df_cluster_coordiante.iloc[0,0]+':'+str(df_cluster_coordiante.iloc[:,1].astype(int).min())+'-'+str(df_cluster_coordiante.iloc[:,2].astype(int).max())\n",
    "            cluster_name.append(cluster_coordinate) ##0: cluster name\n",
    "            cluster_num_vertices.append(len(df_vs_inside_cluster)) # 1: num_vertices\n",
    "    \n",
    "    df_cluster_output = pd.DataFrame(data={'hub_name':cluster_name,'Num_vertices':cluster_num_vertices}).sort_values('Num_vertices', ascending=False)\n",
    "    return df_cluster_output, df_vs_cluster_group\n",
    "\n",
    "\n",
    "def graph_community_multilevel_Blondel_diff_level_promoter(input_graph, cutoff):\n",
    "    ## input graph should have at least one attribute: name\n",
    "    df_vs = convert_graph_vs_to_df(input_graph)\n",
    "    _col_vs_name='name'\n",
    "    \n",
    "    if (input_graph.is_weighted()):\n",
    "        print (\"Weighted Graph Cluster\")\n",
    "        structure = input_graph.community_multilevel(weights=input_graph.es['weight'], return_levels=True)\n",
    "    else:\n",
    "        structure = input_graph.community_multilevel(return_levels=True)\n",
    "    \n",
    "    for tem_level in structure:\n",
    "        print (tem_level.summary())\n",
    "    df_vs['membership'] = structure[0].membership\n",
    "    df_vs_cluster_group = df_vs.groupby('membership')\n",
    "    \n",
    "    ## Rank each cluster by number of bins\n",
    "    cluster_summary = []\n",
    "    for df_vs_cluster in df_vs_cluster_group:\n",
    "        df_cluster = df_vs_cluster[1] \n",
    "        if( len(df_cluster[df_cluster['Promoter']!=0])>0):\n",
    "            for promoter_id in df_cluster[df_cluster['Promoter']!=0]['Promoter_gene_id'].unique():#[0]\n",
    "                #promoter_id = df_cluster[df_cluster['Promoter']!=0]['Promoter_gene_id'].unique()[0]\n",
    "                #print(promoter_id)\n",
    "                if (promoter_id=='Myb'):\n",
    "                    df_test_out = df_cluster\n",
    "                df_vs_inside_cluster, cluster_coordinate = Cluster_Filter_by_Denisty_Promoter(df_cluster, _col_vs_name, promoter_id, cutoff)            \n",
    "                cluster_summary.append( [cluster_coordinate, len(df_vs_inside_cluster), promoter_id])\n",
    "                \n",
    "    \n",
    "    df_cluster_output = pd.DataFrame(data=cluster_summary, columns=['hub_name','Num_vertices','Promoter']).sort_values('Num_vertices', ascending=False)\n",
    "    return df_cluster_output, df_vs_cluster_group, df_test_out\n",
    "\n",
    "def Cluster_Filter_by_Denisty_Promoter(_df_vs_cluster, _col_name, _promoter_id, _cutoff):\n",
    "    ## Linear Denisty Threshold, 1 edge at least 1 anchor\n",
    "    cutoff=_cutoff#0.5\n",
    "    df_tem = _df_vs_cluster\n",
    "    col_name='name'\n",
    "    promoter_id = _promoter_id\n",
    "    resolution=10000\n",
    "    df_tem[col_name].str.split(r\"\\:|-\",expand=True)\n",
    "    df_tem = pd.concat( [df_tem[col_name].str.split(r\"\\:|-\",expand=True),df_tem], axis=1)\n",
    "    \n",
    "    ## Define Target Promoter as core\n",
    "    core = df_tem[df_tem['Promoter_gene_id']==promoter_id].iloc[:,1:3].astype(int).sum(axis=1)/2\n",
    "    #print (core)\n",
    "\n",
    "    df_tem['density'] = df_tem['degree'].astype(float)/(abs(df_tem.iloc[:,1].astype(float)-float(core))**2)*resolution**2\n",
    "    df_filtered_cluster_elements = df_tem[df_tem['density']>cutoff]\n",
    "    \n",
    "    df_cluster_coordiante = df_filtered_cluster_elements[col_name].str.split(r\"\\:|-\",expand=True)\n",
    "    \n",
    "    cluster_coordinate = 'chr'+df_cluster_coordiante.iloc[0,0]+':'+str(df_cluster_coordiante.iloc[:,1].astype(int).min())+'-'+str(df_cluster_coordiante.iloc[:,2].astype(int).max())\n",
    "    \n",
    "    \n",
    "    return df_filtered_cluster_elements, cluster_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_graph_vertex(input_graph, vs_idx_set):\n",
    "    # Input graph and Input vertex index set\n",
    "    for vs in graph_processed.vs.select(vs_idx_set):\n",
    "        print ( \"vs_idx:\"+ str(vs.index)+ ' '+ str(vs.attributes()))\n",
    "    return None\n",
    "def display_graph_edge(input_graph, vs_idx_set):\n",
    "    # Input graph and Input vertex index set\n",
    "    for vs_idx in vs_idx_set:\n",
    "        edges_from_vs = input_graph.es[input_graph.incident(vs_idx)]\n",
    "        for es in edges_from_vs:\n",
    "            print ( \"es_idx:\"+ str(es.index)+ ' '+ str(es.tuple))\n",
    "    return None\n",
    "\n",
    "def annotate_graph_with_feature_values_new(_input_graph, graph_name_col2bed, path_feature, feature_name, _feature_score, norm_factor=1.0):\n",
    "    input_graph = _input_graph\n",
    "    name_col2bed = graph_name_col2bed ## Default \"name\"\n",
    "    Vs_Attrs_Name = feature_name ## such as 'Tcf1'\n",
    "    if ( Vs_Attrs_Name not in input_graph.vs.attributes()):\n",
    "        ## Convert vs to bed format in order to annotate\n",
    "        df_vs_bed = convert_vs2bed(input_graph, name_col2bed)\n",
    "        ### df_vs_bed to be annoted\n",
    "        df_vs_bed.iloc[:,0]='chr'+ df_vs_bed.iloc[:,0].astype(str)\n",
    "        Feature_vs = BedTool.from_dataframe(df_vs_bed).sort()\n",
    "\n",
    "        PATH_Feature_A = path_feature ##\n",
    "        df_A = pd.read_csv(PATH_Feature_A, sep=\"\\t\")\n",
    "        Feature_A = BedTool.from_dataframe(df_A).sort()\n",
    "\n",
    "        ## annotate A in vs\n",
    "        Feature_vs_with_A = Feature_vs.intersect(Feature_A, wb=True, F=0.3) ## 30% maybe enough\n",
    "\n",
    "        if (len(Feature_vs_with_A)>0):\n",
    "            df_vs_with_A=pd.read_csv(Feature_vs_with_A.fn, sep=\"\\t\", names=df_vs_bed.columns.append(df_A.columns).values, header=None)\n",
    "        else:\n",
    "            df_vs_with_A=pd.DataFrame(columns=df_vs_bed.columns.append(df_A.columns))\n",
    "        \n",
    "        \n",
    "        vs_score = _feature_score  ## 'such as logFC'\n",
    "        vs_attrs_score = feature_name+'_'+vs_score\n",
    "        input_graph.vs[Vs_Attrs_Name]=0\n",
    "        input_graph.vs[vs_attrs_score]=0\n",
    "\n",
    "        for df_vs in df_vs_with_A.groupby(name_col2bed): ### Default Define vertex attribute \"name\"\n",
    "            input_graph.vs.select(name=df_vs[0])[Vs_Attrs_Name] = df_vs[1].shape[0]\n",
    "            ### max Tcf1 binding\n",
    "            if ( type(df_vs[1].loc[:,vs_score].head(1).values[0]) == str):\n",
    "                input_graph.vs.select(name=df_vs[0])[vs_attrs_score] = df_vs[1].loc[:,vs_score].max()\n",
    "            else:\n",
    "                #print(df_vs[1].loc[:,vs_score])\n",
    "                if (df_vs[1].shape[0]==1):\n",
    "                    input_graph.vs.select(name=df_vs[0])[vs_attrs_score] = df_vs[1].loc[:,vs_score].values[0]/norm_factor\n",
    "                else:\n",
    "                    List_Feature = list(df_vs[1].loc[:,vs_score].values)\n",
    "                    input_graph.vs.select(name=df_vs[0])[vs_attrs_score] = List2Str(List_Feature, norm_factor)#multiple save as str\n",
    "\n",
    "        print (\"Annotate \" + Vs_Attrs_Name + \" is finished.\")\n",
    "    else: \n",
    "        print (\"Feature of \" + Vs_Attrs_Name + \" is already annoated. Skip.\")\n",
    "    \n",
    "    return input_graph\n",
    "\n",
    "def annotate_graph_with_feature_values(_input_graph, graph_name_col2bed, path_feature, feature_name, _feature_score):\n",
    "    input_graph = _input_graph\n",
    "    name_col2bed = graph_name_col2bed ## Default \"name\"\n",
    "    Vs_Attrs_Name = feature_name ## such as 'Tcf1'\n",
    "    if (Vs_Attrs_Name not in _input_graph.vs.attributes()):\n",
    "        ## Convert vs to bed format in order to annotate\n",
    "        df_vs_bed = convert_vs2bed(input_graph, name_col2bed)\n",
    "        ### df_vs_bed to be annoted\n",
    "        Feature_vs = BedTool.from_dataframe(df_vs_bed).sort()\n",
    "\n",
    "        PATH_Feature_A = path_feature ##\n",
    "        df_A = pd.read_csv(PATH_Feature_A, sep=\"\\t\")\n",
    "        Feature_A = BedTool.from_dataframe(df_A).sort()\n",
    "\n",
    "        ## annotate A in vs\n",
    "        Feature_vs_with_A = Feature_vs.intersect(Feature_A, wb=True, F=0.3)\n",
    "\n",
    "        if (len(Feature_vs_with_A)>0):\n",
    "            df_vs_with_A=pd.read_csv(Feature_vs_with_A.fn, sep=\"\\t\", names=df_vs_bed.columns.append(df_A.columns).values, header=None)\n",
    "        else:\n",
    "            df_vs_with_A=pd.DataFrame(columns=df_vs_bed.columns.append(df_A.columns))\n",
    "        \n",
    "        \n",
    "        vs_score = _feature_score  ## 'such as logFC'\n",
    "        vs_attrs_score = Vs_Attrs_Name+'_'+vs_score\n",
    "        input_graph.vs[Vs_Attrs_Name]=0\n",
    "        input_graph.vs[vs_attrs_score]=0\n",
    "        for df_vs in df_vs_with_A.groupby(name_col2bed): ### Default Define vertex attribute \"name\"\n",
    "            input_graph.vs.select(name=df_vs[0])[Vs_Attrs_Name] = df_vs[1].shape[0]\n",
    "            ### max Tcf1 binding\n",
    "            if ( type(df_vs[1].loc[:,vs_score].head(1).values[0]) == str):\n",
    "                input_graph.vs.select(name=df_vs[0])[vs_attrs_score] = df_vs[1].loc[:,vs_score].max()\n",
    "            else:\n",
    "                #print(df_vs[1].loc[:,vs_score])\n",
    "                input_graph.vs.select(name=df_vs[0])[vs_attrs_score] = df_vs[1].loc[:,vs_score].mean()\n",
    "        print (\"Annotate \" + Vs_Attrs_Name + \" is finished.\")\n",
    "    else: \n",
    "        print (\"Feature of \" + Vs_Attrs_Name + \" is already annoated. Skip.\")\n",
    "    \n",
    "    return input_graph\n",
    "\n",
    "def Return_Graph_of_Gene(_input_graph, _gene, _search_depth):\n",
    "    search_depth=_search_depth\n",
    "    graph_input = _input_graph\n",
    "    gene_request = _gene\n",
    "    vertex_set=set()\n",
    "    if ( len(graph_input.vs.select(Promoter_gene_id=gene_request))>0 ):\n",
    "        vertex_set.add(graph_input.vs.select(Promoter_gene_id=gene_request)[0].index)\n",
    "        final_subgrapph=None\n",
    "        for i in range(search_depth):\n",
    "            for vertex_index in list(vertex_set):\n",
    "                graph_select_edges = graph_input.es[graph_input.incident(vertex_index)]\n",
    "                for edge_in_graph in graph_select_edges:\n",
    "                    vertex_set.add(edge_in_graph.tuple[0])\n",
    "                    vertex_set.add(edge_in_graph.tuple[1])\n",
    "\n",
    "            #print (vertex_set)\n",
    "            final_subgrapph = graph_input.induced_subgraph(vertex_set)\n",
    "    else:\n",
    "        print ('Gene Not Included!')\n",
    "        final_subgrapph=None\n",
    "    \n",
    "    return final_subgrapph\n",
    "\n",
    "def Visualization_Graph_bp(_sub_graph_test, _plot_name):\n",
    "    ### Mark Promoter in graph with gene_id\n",
    "    sub_graph_test = _sub_graph_test\n",
    "    #sub_graph_test.vs.select(Promoter_gt=0)['label'] = sub_graph_test.vs.select(Promoter_gt=0)['Promoter_gene_id']\n",
    "\n",
    "    #color_dict = {\"Tcf1\":\"red\", \"None\": \"gray\"}\n",
    "    #shape_dict ={'Promoter': 'circular', 'Enhancer':'arrow-up', 'Enhancer_Down':'arrow-down',\n",
    "    #'Enhancer_Up':'arrow-up', 'None':'hidden'} \n",
    "\n",
    "## Default\n",
    "    sub_graph_test.vs[\"color\"] = \"gray\"\n",
    "    #sub_graph_test.vs[\"shape\"] = \"hidden\"\n",
    "\n",
    "    # high priority will be put in the end\n",
    "    if ( 'K27ac' in sub_graph_test.vs.attributes()):\n",
    "        sub_graph_test.vs.select(K27ac_gt=0)[\"shape\"] = 'diamond'\n",
    "        #sub_graph_test.vs.select(K27ac_gt=0)['label'] = [\"{:.2f}\".format(x) for x in sub_graph_test.vs.select(K27ac_gt=0)['K27ac_logFC']]\n",
    "    \n",
    "    ## DNase Priority 2\n",
    "    if ( 'DNase' in sub_graph_test.vs.attributes()):\n",
    "        sub_graph_test.vs.select(DNase_FC_logFC_gt=0)[\"shape\"] = 'arrow-up'\n",
    "        sub_graph_test.vs.select(DNase_FC_logFC_lt=0)[\"shape\"] = 'arrow-down'\n",
    "        sub_graph_test.vs.select(DNase_gt=0)[\"size\"] = [abs(x) * 20 for x in sub_graph_test.vs.select(DNase_gt=0)['DNase_FC_logFC']]\n",
    "    \n",
    "    ## Promoter Priority 1\n",
    "    if ( 'Promoter' in sub_graph_test.vs.attributes()):\n",
    "        sub_graph_test.vs.select(Promoter_gt=0)[\"shape\"] = 'circular'\n",
    "        sub_graph_test.vs.select(Promoter_gt=0)['label'] = sub_graph_test.vs.select(Promoter_gt=0)['Promoter_gene_id']\n",
    "        sub_graph_test.vs.select(Promoter_gt=0)[\"size\"] = 20\n",
    "        ## convert Genomic Region to shape code \n",
    "    if ( 'Tcf1' in sub_graph_test.vs.attributes()):\n",
    "        sub_graph_test.vs.select(Tcf1_gt=0)[\"color\"] = \"red\"\n",
    "\n",
    "    sub_graph_test.vs['name']=''\n",
    "    graph_plot_saveas(sub_graph_test, _plot_name+'.PNG')\n",
    "    \n",
    "    return sub_graph_test\n",
    "### End of Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ABC_in_given_gene_and_graph(_gene_id, _graph, _col1, _col2, _col3_interaction):\n",
    "    ## Define activity name\n",
    "    A_name1= _col1\n",
    "    A_name2= _col2\n",
    "    geneid=_gene_id\n",
    "    return_graph = Return_Graph_of_Gene(_graph, geneid, 1)\n",
    "\n",
    "    for vs_idx in return_graph.vs.select(Promoter_gene_id_eq=geneid):\n",
    "        ## All edges has a connect with promoter\n",
    "        edges_from_vs = return_graph.es[return_graph.incident(vs_idx)]\n",
    "        promoter_index=vs_idx.index\n",
    "        Sum_All_edges=list()\n",
    "        for es in edges_from_vs:\n",
    "            for index_anchor in es.tuple:\n",
    "                if (index_anchor == vs_idx.index):\n",
    "                    continue\n",
    "                else:\n",
    "                    #print (index_anchor)\n",
    "                    Contact_interaction = es[_col3_interaction]\n",
    "                    Activity_E = np.sqrt(return_graph.vs[index_anchor][A_name1]*return_graph.vs[index_anchor][A_name2])\n",
    "                    Sum_All_edges.append( [geneid, return_graph.vs[index_anchor]['name'], Activity_E, Contact_interaction])\n",
    "\n",
    "        ## Calculate ABC\n",
    "        df_sum_all_edges=pd.DataFrame(data=Sum_All_edges, columns=['gene_id', 'loc', 'Activity_E', 'Contact_interaction'])\n",
    "        df_sum_all_edges['AC_Score'] = (df_sum_all_edges['Activity_E']*df_sum_all_edges['Contact_interaction'])/sum(0.01+df_sum_all_edges['Activity_E']*df_sum_all_edges['Contact_interaction'])\n",
    "\n",
    "    return df_sum_all_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_All_Genes(Input_Path, number):\n",
    "#### READ FILE\n",
    "    df = pd.read_csv(Input_Path+'/gene_exp.diff', sep='\\t', header=0, usecols={'test_id',\\\n",
    "    'status','sample_1','sample_2','value_1','value_2','log2(fold_change)','p_value','q_value'})\n",
    "#### Rename columns\n",
    "    df=df.rename(columns={'test_id':'gene_id', 'value_1': df['sample_1'].unique()[0], 'value_2': df['sample_2'].unique()[0]})\n",
    "#### Output   \n",
    "    return df.loc[:,['gene_id',df['sample_1'].unique()[0],df['sample_2'].unique()[0],'log2(fold_change)',\n",
    "                     'p_value','q_value', 'End_'+str(number), '||']].fillna('')\n",
    "\n",
    "def Predict_expression_mode1(input_graph):\n",
    "    ## Input Graph must have attributes: \"region\", \"binding\"\n",
    "    vs_promoter = input_graph.vs.select(region=\"Promoter\")\n",
    "    output_gene_with_score=list()\n",
    "    for vertex in vs_promoter:\n",
    "        edges_from_vs = input_graph.es[input_graph.incident(vertex.index)]\n",
    "        vs_subgraph=edges_from_vs.subgraph()\n",
    "        test_score = 0 ## for a single promoter vertex define a expression score\n",
    "        for vs_region, vs_binding in zip(vs_subgraph.vs['region'], vs_subgraph.vs['binding']):\n",
    "            #print (vs_region,vs_binding)\n",
    "            if (vs_region == 'Enhancer_Down'): test_score+=(-1)\n",
    "            elif (vs_region == 'Enhancer_Up'): test_score+=1\n",
    "            if ( vs_binding=='Tcf1'): test_score*=2\n",
    "            ## end\n",
    "        output_gene_with_score.append( [vertex['label'], test_score])\n",
    "    return output_gene_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Bedpe to graph\n",
    "## Input format for interaction should be the simplest\n",
    "## chr b1 b2 interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/.conda/envs/py3_lx/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3101646, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Read_Interaction(_PATH_interaction, _resolution, _col_fore, _col_back):\n",
    "    PATH_interaction=_PATH_interaction\n",
    "    col_fore = _col_fore\n",
    "    col_back  = _col_back\n",
    "    resolution = _resolution\n",
    "    \n",
    "    df_interaction = pd.read_csv(PATH_interaction, sep=\"\\t\").fillna(0)\n",
    "    df_interaction = df_interaction[df_interaction.iloc[:,1]!=df_interaction.iloc[:,2]] ### remove self interaction\n",
    "    df_interaction.loc[:,'#chr']=df_interaction.loc[:,'#chr'].replace('chr','')\n",
    "    df_interaction.loc[:,'#chr1']=df_interaction.iloc[:,0]\n",
    "    df_interaction.loc[:,'x1']=df_interaction.iloc[:,1]\n",
    "    df_interaction.loc[:,'x2']=df_interaction.iloc[:,1]+resolution\n",
    "    df_interaction.loc[:,'chr2']=df_interaction.iloc[:,0]\n",
    "    df_interaction.loc[:,'y1']=df_interaction.iloc[:,2]\n",
    "    df_interaction.loc[:,'y2']=df_interaction.iloc[:,2]+resolution\n",
    "\n",
    "    df_interaction.loc[:,'log_FC'] = np.log2(df_interaction.loc[:,col_fore].replace(0,0.1) / df_interaction.loc[:,col_back].replace(0,0.1) )\n",
    "    #df_interaction.loc[:,'GeneID'] = \"id_\"+df_interaction.index.astype(str)\n",
    "    df_interaction = df_interaction.loc[:,['#chr1','x1','x2','chr2','y1','y2','log_FC', col_fore, col_back]]\n",
    "    return df_interaction\n",
    "col_fore = 'DKO_na'\n",
    "col_back  = 'WT_na'\n",
    "resolution = 10000\n",
    "\n",
    "PATH_FEATURES='/home/xli/Data/Shaoqi/Hub/'\n",
    "PATH_interaction=PATH_FEATURES+'WT_na-DKO_na_2016.bed'\n",
    "\n",
    "df_hic = Read_Interaction(PATH_FEATURES+'WT_na-DKO_na_2016.bed', resolution, col_fore, col_back)\n",
    "df_hic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/.conda/envs/py3_lx/lib/python3.7/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/xli/.conda/envs/py3_lx/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IGRAPH UNW- 251764 1397594 -- \\n+ attr: name (v), DKO_na (e), weight (e)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_WT_Specific_loops  = df_hic[(df_hic['log_FC']<0)]#&(df_interaction['DKO']!=0.1)] very critical for 19\n",
    "df_WT_Specific_loops.loc[:, 'logFC'] = abs(df_WT_Specific_loops.loc[:,'log_FC'])\n",
    "\n",
    "g_graph = convert_loops_to_graph(df_WT_Specific_loops, col_back, col_fore)\n",
    "g_graph.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Graph Cluster\n",
      "Clustering with 251764 elements and 56658 clusters\n",
      "Clustering with 251764 elements and 7590 clusters\n",
      "Clustering with 251764 elements and 1148 clusters\n",
      "Clustering with 251764 elements and 365 clusters\n",
      "Clustering with 251764 elements and 315 clusters\n"
     ]
    }
   ],
   "source": [
    "graph_processed = g_graph\n",
    "graph_processed = Graph_Pagerank(graph_processed)\n",
    "\n",
    "df_WT_Hub, df_WT_Hub_Groups = graph_community_multilevel_Blondel_diff_level(graph_processed, 0.5)\n",
    "\n",
    "df_WT_Hub_top = convert_cluster2bed(df_WT_Hub, 'hub_name').reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/.conda/envs/py3_lx/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/xli/.conda/envs/py3_lx/lib/python3.7/site-packages/scipy/stats/morestats.py:2863: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "## Calculate pvalue for each hub\n",
    "\n",
    "df_Hub_filter = convert_cluster2bed(df_WT_Hub, 'hub_name').reset_index().drop('index', axis=1)\n",
    "\n",
    "## Associated each Hub with interaction and pvalue\n",
    "col_name = df_hic.columns\n",
    "df_inter = pd.read_csv(PATH_interaction, sep=\"\\t\").fillna(0)\n",
    "df_inter = df_inter[df_inter.iloc[:, 1]!=df_inter.iloc[:, 2]]\n",
    "df_inter.loc[:,'#chr']= 'chr'+df_inter.loc[:,'#chr'].astype(str)\n",
    "Feature_interaction = BedTool.from_dataframe(df_inter).sort()\n",
    "Feature_hub = BedTool.from_dataframe(df_Hub_filter).sort()\n",
    "#\n",
    "Feature_Hub_interaction = Feature_hub.intersect(Feature_interaction, wa=True, wb=True, F=1.0)\n",
    "\n",
    "col_name = df_Hub_filter.columns.append(df_inter.columns)\n",
    "df_Feature_Hub_interaction = pd.read_csv(Feature_Hub_interaction.fn, sep='\\t', names=col_name)\n",
    "\n",
    "df_Feature_Hub_interaction_group = df_Feature_Hub_interaction.groupby('hub_name')\n",
    "\n",
    "\n",
    "\n",
    "### calculate a pvalue for each hub\n",
    "hub_sum=[]\n",
    "for hub in df_Feature_Hub_interaction_group:\n",
    "    #print (hub[0])\n",
    "    df_hub = hub[1]\n",
    "    data_for_test = df_hub.loc[:, col_back]  - df_hub.loc[:, col_fore]\n",
    "    w, pvalue_hub = wilcoxon(data_for_test)#, alternative='less')\n",
    "    hub_sum.append([hub[0], df_hub.Num_vertices.unique()[0], pvalue_hub])\n",
    "    #break\n",
    "\n",
    "df_hub_summary = pd.DataFrame( data = hub_sum, columns=['hub_name', 'Num_vertices', 'pvalue'])\n",
    "df_hub_summary = df_Hub_filter.merge(df_hub_summary, on=['hub_name','Num_vertices'], how='inner').sort_values(by='pvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>hub_name</th>\n",
       "      <th>Num_vertices</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>chr18</td>\n",
       "      <td>56520000</td>\n",
       "      <td>56660000</td>\n",
       "      <td>chr18:56520000-56660000</td>\n",
       "      <td>14</td>\n",
       "      <td>1.077923e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>chr15</td>\n",
       "      <td>36260000</td>\n",
       "      <td>36410000</td>\n",
       "      <td>chr15:36260000-36410000</td>\n",
       "      <td>14</td>\n",
       "      <td>1.614891e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>chr10</td>\n",
       "      <td>20850000</td>\n",
       "      <td>20990000</td>\n",
       "      <td>chr10:20850000-20990000</td>\n",
       "      <td>14</td>\n",
       "      <td>1.869677e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>chr11</td>\n",
       "      <td>6540000</td>\n",
       "      <td>6660000</td>\n",
       "      <td>chr11:6540000-6660000</td>\n",
       "      <td>12</td>\n",
       "      <td>9.463384e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>chr13</td>\n",
       "      <td>44840000</td>\n",
       "      <td>44970000</td>\n",
       "      <td>chr13:44840000-44970000</td>\n",
       "      <td>13</td>\n",
       "      <td>1.550791e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chr11</td>\n",
       "      <td>53930000</td>\n",
       "      <td>54080000</td>\n",
       "      <td>chr11:53930000-54080000</td>\n",
       "      <td>15</td>\n",
       "      <td>1.672322e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>chr10</td>\n",
       "      <td>84320000</td>\n",
       "      <td>84450000</td>\n",
       "      <td>chr10:84320000-84450000</td>\n",
       "      <td>13</td>\n",
       "      <td>2.905960e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>chr7</td>\n",
       "      <td>74570000</td>\n",
       "      <td>74660000</td>\n",
       "      <td>chr7:74570000-74660000</td>\n",
       "      <td>8</td>\n",
       "      <td>3.073152e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>chr6</td>\n",
       "      <td>28210000</td>\n",
       "      <td>28330000</td>\n",
       "      <td>chr6:28210000-28330000</td>\n",
       "      <td>12</td>\n",
       "      <td>4.312522e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>chr13</td>\n",
       "      <td>43460000</td>\n",
       "      <td>43580000</td>\n",
       "      <td>chr13:43460000-43580000</td>\n",
       "      <td>12</td>\n",
       "      <td>5.218285e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2                 hub_name  Num_vertices  \\\n",
       "56    chr18  56520000  56660000  chr18:56520000-56660000            14   \n",
       "26    chr15  36260000  36410000  chr15:36260000-36410000            14   \n",
       "51    chr10  20850000  20990000  chr10:20850000-20990000            14   \n",
       "195   chr11   6540000   6660000    chr11:6540000-6660000            12   \n",
       "111   chr13  44840000  44970000  chr13:44840000-44970000            13   \n",
       "19    chr11  53930000  54080000  chr11:53930000-54080000            15   \n",
       "79    chr10  84320000  84450000  chr10:84320000-84450000            13   \n",
       "1727   chr7  74570000  74660000   chr7:74570000-74660000             8   \n",
       "163    chr6  28210000  28330000   chr6:28210000-28330000            12   \n",
       "153   chr13  43460000  43580000  chr13:43460000-43580000            12   \n",
       "\n",
       "            pvalue  \n",
       "56    1.077923e-13  \n",
       "26    1.614891e-12  \n",
       "51    1.869677e-11  \n",
       "195   9.463384e-09  \n",
       "111   1.550791e-08  \n",
       "19    1.672322e-07  \n",
       "79    2.905960e-07  \n",
       "1727  3.073152e-07  \n",
       "163   4.312522e-07  \n",
       "153   5.218285e-07  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hub_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
