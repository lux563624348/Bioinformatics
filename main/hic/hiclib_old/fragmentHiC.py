# (c) 2012 Massachusetts Institute of Technology. All Rights Reserved
# Code written by: Maksim Imakaev (imakaev@mit.edu)
"""
This is a module class for fragment-level Hi-C data analysis.
The base class "HiCdataset" can load, save and merge Hi-C datasets,
perform certain filters, and save binned heatmaps.

Additional class HiCStatistics contains methods to analyze HiC data
on a fragment level.
This includes read statistics, scalings, etc.

Input data
----------

When used together with iterative mapping, this class can load
files from h5dicts created by iterative mapping.

This method can also input any dictionary-like structure, such as a dictionary,
np.savez, etc. The minimal subset of information are positions of two reads,
but providing strand informations is adviced.

If restriction fragment assignment is not provided,
it will be automatically recalculated.

.. warning ::
    1-bp difference in positions of restriction sites will force certain
    algorithms, such as scaling calculations, to throw an exception. It is
    adviced to supply restriction site data only if it was generated by
    iterative mapping code.

Concepts
--------

All read data is stored in a synchronized h5dict-based dictionary of arrays.
Each variable has a fixed name and type, as specified in the self.vectors
variable. Whenever the variable is accessed from the program, it is loaded from
the h5dict.


Whenever a set of reads needs to be excluded from the dataset, a
:py:func:`maskFilter  <HiCdataset.maskFilter>` method is called,
that goes over all datasets and overrides them.
This method automatically rebuilds fragments.

Filtering the data
------------------

This class has many build-in methods for filtering the data.
However, one can easily construct another filter as presented in
multiple one-liner examples below

.. code-block:: python

    >>> Dset = HiCdataset(**kwargs)
    >>> Dset.fragmentFilter((Dset.ufragmentlen >1000) * \
    (Dset.ufragmentlen < 4000))
    #Keep reads from fragments between 1kb and 4kb long.

    >>> Dset.maskFilter(Dset.chrms1 == Dset.chrms2)  #keep only cis reads

    >>> Dset.maskFilter((Dset.chrms1 !=14) + (Dset.chrms2 !=14))
    #Exclude all reads from chromosome 15 (yes, chromosomes are zero-based!)

    >>> Dset.maskFilter(Dset.dist1 + Dset.dist2 > 500)
    #Keep only random breaks, if 500 is maximum molecule length

-------------------------------------------------------------------------------

API documentation
-----------------



"""

from __future__ import absolute_import, division, print_function, unicode_literals
import warnings
import os
import six
import traceback
from copy import copy
from mirnylib.genome import Genome
import numpy as np
import gc
import pickle
from hiclib.hicShared import binarySearch, sliceableDataset, mydtype, h5dictBinarySearch, mydtypeSorter, searchsorted
import mirnylib.h5dict
from mirnylib import numutils
from mirnylib.numutils import arrayInArray, \
    uniqueIndex, fasterBooleanIndexing, fillDiagonal, arraySearch, \
    arraySumByArray, externalMergeSort, chunkedBincount
import time
from textwrap import dedent
from mirnylib.systemutils import setExceptionHook
from statsmodels.sandbox.distributions.transformed import absfunc
USE_NUMEXPR = True
import numexpr
import logging
log = logging.getLogger(__name__)


class HiCdataset(object):
    """Base class to operate on HiC dataset.

    This class stores all information about HiC reads on a hard drive.
    Whenever a variable corresponding to any record is used,
    it is loaded/saved from/to the HDD.

    If you apply any filters to a dataset, it will actually modify
    the content of the current working file.
    Thus, to preserve the data, loading datasets is advised. """

    def __init__(self, filename, genome, enzymeName="fromGenome", maximumMoleculeLength=500,
                 inMemory=False, mode="a", tmpFolder="/tmp", dictToStoreIDs="dict"):
        """
        __init__ method

        Initializes empty dataset by default.
        If "override" is False, works with existing dataset.

        Parameters
        ----------
        filename : string
            A filename to store HiC dataset in an HDF5 file.
        genome : folder with genome, or Genome object
            A folder with fastq files of the genome
            and gap table from Genome browser.
            Alternatively, mirnylib.genome.Genome object.
        maximumMoleculeLength : int, optional
            Maximum length of molecules in the HiC library,
            used as a cutoff for dangling ends filter
        inMemory : bool, optional
            Create dataset in memory. Filename is ignored then,
            but still needs to be specified.
        mode : str
            'r'  - Readonly, file must exist

            'r+' - Read/write, file must exist

            'w'  - Create file, overwrite if exists

            'w-' - Create file, fail if exists

            'a'  - Read/write if exists, create otherwise (default)
        dictToStoreIDs : dict-like or "dict" or "h5dict"
            A dictionary to store rsite IDs. If "dict", then store them in memory.
            If "h5dict", then creates default h5dict (in /tmp folder)
            If other object, uses it, whether it is an h5dict or a dictionary
        """
        # -->>> Important::: do not define any variables before vectors!!! <<<--
        # These are fields that will be kept on a hard drive
        # You can learn what variables mean from here too.
        self.vectors = {
            # chromosomes for each read.
            "strands1": "bool", "strands2": "bool",  #strand to which the read maps
            "chrms1": "int16", "chrms2": "int16",   # chromosome to which the read maps
            "cuts1": "int32", "cuts2": "int32",     #Start of the read ("ultrasonic" cut site)

            }
        self.vectors2 = {
             "fraglens1": "int32", "fraglens2": "int32", # fragment lengthes
            "fragids1": "int64", "fragids2": "int64",    # fragid as defined in the manual
            "mids1": "int32", "mids2": "int32",          # midpoint of a fragment, determined as "(start+end)/2"
            "dists1": "int32", "dists2": "int32",        # distance from a cut site to the restriction fragment
            "distances": "int32",      # distance between fragments. If -1, different chromosomes. If -2, different arms.
            }
        self.vectors3 = {
            #absolute ID of a restriction fragment (0,1,2,... numFragments)
            "rfragAbsIdxs1": "int32", "rfragAbsIdxs2": "int32", }
        if dictToStoreIDs == "dict":
            self.rfragIDDict = {}
        elif dictToStoreIDs == "h5dict":
            self.rfragIDDict = mirnylib.h5dict.h5dict()
        else:
            self.rfragIDDict = dictToStoreIDs

        self.metadata = {}
        self.tmpDir = tmpFolder
        if not os.path.exists(self.tmpDir):
            os.makedirs(self.tmpDir)


        #-------Initialization of the genome and parameters-----
        self.mode = mode
        if isinstance(genome, six.string_types):
            self.genome = Genome(genomePath=genome, readChrms=["#", "X"])
        else:
            self.genome = genome
        assert isinstance(self.genome, Genome)  # bla
        if enzymeName == "fromGenome":
            if self.genome.hasEnzyme() == False:
                raise ValueError("Provide the genome with the enzyme or specify enzyme=...")
        else:
            self.genome.setEnzyme(enzymeName)

        self.chromosomeCount = self.genome.chrmCount
        self.fragIDmult = self.genome.fragIDmult  # used for building heatmaps
        self.rFragIDs = self.genome.rfragMidIds
        self.rFragLens = np.concatenate(self.genome.rfragLens)
        self.rFragMids = np.concatenate(self.genome.rfragMids)
        self.rsites = self.genome.rsiteIds
        # to speed up searchsorted we use positive-only numbers
        self.rsitesPositive = self.rsites + 2 * self.fragIDmult


        print("----> New dataset opened, genome %s, filename = %s" % (
            self.genome.folderName, filename))

        self.maximumMoleculeLength = maximumMoleculeLength
        # maximum length of a molecule for SS reads

        self.filename = os.path.abspath(os.path.expanduser(filename))  # File to save the data
        self.chunksize = 10000000
        # Chunk size for h5dict operation, external sorting, etc

        def dummyFunction(*args, **kwargs):
            pass
        self.maskFilterCallback = dummyFunction

        self.inMemory = inMemory

        #------Creating filenames, etc---------
        if not self.inMemory:
            if os.path.exists(self.filename) and (mode in ['w', 'a']):
                print('----->!!!File already exists! It will be {0}\n'.format(
                    {"w": "deleted", "a": "opened in the append mode"}[mode]))

            if len(os.path.split(self.filename)[0]) != 0:
                if not os.path.exists(os.path.split(self.filename)[0]):
                    warnings.warn("Folder in which you want to create file"
                            "do not exist: %s" % os.path.split(self.filename)[0])
                    try:
                        os.mkdir(os.path.split(self.filename)[0])
                    except:
                        raise IOError("Failed to create directory: %s" %
                                      os.path.split(self.filename)[0])

        self.h5dict = mirnylib.h5dict.h5dict(self.filename, mode=mode, in_memory=inMemory)
        if "chrms1" in list(self.h5dict.keys()):
            self.N = len(self.h5dict.get_dataset("chrms1"))
        if "metadata" in self.h5dict:
            self.metadata = self.h5dict["metadata"]


    def _setData(self, name, data):
        "an internal method to save numpy arrays to HDD quickly"
        if name not in list(self.vectors.keys()):
            raise ValueError("Attept to save data not "
                             "specified in self.vectors")
        dtype = np.dtype(self.vectors[name])
        data = np.asarray(data, dtype=dtype)
        self.h5dict[name] = data

    def _getData(self, name):
        "an internal method to load numpy arrays from HDD quickly"
        if name not in list(self.vectors.keys()):
            raise ValueError("Attept to load data not "
                             "specified in self.vectors")
        return self.h5dict[name]

    def _isSorted(self):
        if self.N == 0:
            return False

        ch1 = self._getVector("chrms1", 0, min(self.N, 10000))
        cs = np.sort(ch1)
        if np.sum(ch1 != cs) == 0:
            c1 = self._getVector("cuts1", 0, min(self.N, 10000))
            cs = np.sort(ch1)
            if np.sum(ch1 != cs) == 0:
                c2 = self._getVector("cuts2", 0, min(self.N, 10000))
                ch2 = self._getVector("chrms2", 0, min(self.N, 10000))
                if (c2 < c1)[ch1 == ch2].sum() == 0:

                    return True
        return False

    def __getattribute__(self, x):
        """a method that overrides set/get operation for self.vectors
        o that they're always on HDD"""
        if x in ["vectors", "vectors2", "vectors3"]:
            return object.__getattribute__(self, x)

        if x in list(self.vectors.keys()):
            a = self._getData(x)
            return a
        elif (x in self.vectors2) or (x in self.vectors3):
            return self._getVector(x)
        else:
            return object.__getattribute__(self, x)

    def _getSliceableVector(self, name):
        return sliceableDataset(self._getVector, name, self.N)

    def _getVector(self, name, start=None, end=None):
        if self.N == 0:
            return []
        if (type(start) == int) and (type(end) == int) and (start == end):
            warnings.warn(RuntimeWarning("Zero length vector requested"))
            return np.array([])
        if name in self.vectors:
            if name in self.h5dict:
                assert self.N == len(self.h5dict.get_dataset(name))
                return self.h5dict.get_dataset(name)[start:end]
            else:
                raise ValueError("name {0} not in h5dict {1}".format(name, self.h5dict.path))

        if name in self.vectors3:
            datas = self.rfragIDDict
            if name not in datas:
                self._calculateRgragIDs()
            assert name in datas
            if hasattr(datas, "get_dataset"):
                dset = datas.get_dataset(name)
            else:
                dset = datas[name]
            return dset[start:end]

        if name in self.vectors2:
            if name == "fragids1":
                return self.genome.rfragMidIds[self._getVector("rfragAbsIdxs1", start, end)]
            elif name == "fragids2":
                return self.genome.rfragMidIds[self._getVector("rfragAbsIdxs2", start, end)]
            elif name == "fraglens1":
                fl1 = self.rFragLens[self._getVector("rfragAbsIdxs1", start, end)]
                fl1[self._getVector("chrms1", start, end) == -1] = 0
                return fl1
            elif name == "fraglens2":
                fl2 = self.rFragLens[self._getVector("rfragAbsIdxs2", start, end)]
                fl2[self._getVector("chrms2", start, end) == -1] = 0
                return fl2
            elif name == "dists1":
                cutids1 = self._getVector("cuts1", start, end) + np.array(self._getVector("chrms1", start, end), dtype=np.int64) * self.fragIDmult
                d1 = np.abs(cutids1 - self.rsites[self._getVector("rfragAbsIdxs1", start, end) + self._getVector("strands1", start, end) - 1])
                d1[self._getVector("chrms1", start, end) == -1] = 0
                return d1
            elif name == "dists2":
                cutids2 = self._getVector("cuts2", start, end) + np.array(self._getVector("chrms2", start, end), dtype=np.int64) * self.fragIDmult
                d2 = np.abs(cutids2 - self.rsites[self._getVector("rfragAbsIdxs2", start, end) + self._getVector("strands2", start, end) - 1])
                d2[self._getVector("chrms2", start, end) == -1] = 0
                return d2

            elif name == "mids1":
                return self.rFragMids[self._getVector("rfragAbsIdxs1", start, end)]
            elif name == "mids2":
                return self.rFragMids[self._getVector("rfragAbsIdxs2", start, end)]
            elif name == "distances":
                dvec = np.abs(self._getVector("mids1", start, end) - self._getVector("mids2", start, end))
                trans_mask = (self._getVector('chrms1', start, end) != self._getVector('chrms2', start, end))
                dvec[trans_mask] = -1
                return dvec

        raise "unknown vector: {0}".format(name)

    def _calculateRgragIDs(self):
        log.debug("Started calculating rfrag IDs")
        for i in list(self.rfragIDDict.keys()):
            del self.rfragIDDict[i]
        if hasattr(self.rfragIDDict, "add_empty_dataset"):
            self.rfragIDDict.add_empty_dataset("rfragAbsIdxs1", (self.N,), "int32")
            self.rfragIDDict.add_empty_dataset("rfragAbsIdxs2", (self.N,), "int32")
            d1 = self.rfragIDDict.get_dataset("rfragAbsIdxs1")
            d2 = self.rfragIDDict.get_dataset("rfragAbsIdxs2")
        else:
            self.rfragIDDict["rfragAbsIdxs1"] = np.empty(self.N, dtype=np.int32)
            self.rfragIDDict["rfragAbsIdxs2"] = np.empty(self.N, dtype=np.int32)
            d1 = self.rfragIDDict["rfragAbsIdxs1"]
            d2 = self.rfragIDDict["rfragAbsIdxs2"]

        constants = {"np":np, "binarySearch":binarySearch,
                                 "rsites":self.rsitesPositive, "fragMult":self.fragIDmult,
                                 "numexpr":numexpr}

        code1 = dedent("""
        id1 = numexpr.evaluate("(cuts1 + (chrms1+2)  * fragMult + 7 * strands1 - 3)")
        del cuts1
        del chrms1
        res = binarySearch(id1 ,rsites)
        """)
        self.evaluate(code1, ["chrms1", "strands1", "cuts1"], outVariable=("res", d1),
                      constants=constants, chunkSize=150000000)

        code2 = dedent("""
        id2 = numexpr.evaluate("(cuts2 + (chrms2 + 2) * fragMult + 7 * strands2 - 3) * (chrms2 >=0)")
        del cuts2
        del chrms2
        res = binarySearch(id2 ,rsites)
        """)
        self.evaluate(code2, ["chrms2", "strands2", "cuts2"], outVariable=("res", d2),
                      constants=constants, chunkSize=150000000)
        log.debug("Finished calculating rfrag IDs")




    def __setattr__(self, x, value):
        """a method that overrides set/get operation for self.vectors
        so that they're always on HDD"""
        if x in  ["vectors", "vectors2"]:
            return object.__setattr__(self, x, value)

        if x in list(self.vectors.keys()):
            self._setData(x, value)
        elif x in self.vectors2:
            raise
        else:
            return object.__setattr__(self, x, value)


    def _dumpMetadata(self):

        if self.mode in ["r"]:
            warnings.warn(RuntimeWarning("Cannot dump metadata in read mode"))
            return

        try:
            self.h5dict["metadata"] = self.metadata

        except Exception as err:
            print("-" * 20 + "Got Exception when saving metadata" + "-" * 20)
            traceback.print_exc()
            print(Exception, err)
            print("-" * 60)
            warnings.warn(RuntimeWarning("Got exception when saving metadata"))

    def _checkConsistency(self):
        """
        Internal method to automatically check consistency with the genome
        Every time rebuildFragments is getting called
        """
        c1 = self.chrms1
        p1 = self.cuts1
        if len(c1) > 1000000:
            c1 = c1[::100]
            p1 = p1[::100]
        if c1.max() >= self.genome.chrmCount:
            print('Genome length', self.genome.chrmCount)
            print("Maximum chromosome", c1.max())
            print("note that chromosomes are 0-based, so go", end=' ')
            print("from 0 to {0}".format(self.genome.chrmCount))
            raise ValueError("Chromosomes do not fit in the genome")
        maxPos = self.genome.chrmLens[c1]
        dif = p1 - maxPos
        if dif.max() > 0:
            print("Some reads map after chromosome end")
            print('However, deviation  of {0} is not big enough to call an error'.format(dif.max()))
            warnings.warn("Reads map {0} bp after the end of chromosome".format(dif.max()))
            if dif.max() > 100:
                print("Possible genome mismatch found")
                print('Maximum deviation is {0}'.format(dif.max()))
                for chrom in range(self.genome.chrmCount):
                    posmax = (p1[c1 == chrom]).max()
                    chrLens = self.genome.chrmLens[chrom]
                    if posmax > chrLens:
                        print("Maximum position for chr {0} is {1}".format(chrom, posmax))
                        print("Length of chr {0} is {1}".format(chrom, chrLens))
                raise ValueError("Wrong chromosome lengths")

    def _getChunks(self, chunkSize="default"):
        if chunkSize == "default":
            chunkSize = self.chunksize
        if chunkSize > 0.5 * self.N:
            return [(0, self.N)]
        points = list(range(0, self.N - chunkSize // 2, chunkSize)) + [self.N]
        return list(zip(points[:-1], points[1:]))

    def _sortData(self):
        """
        Orders data such that chrms1 is always more than chrms2, and sorts it by chrms1, cuts1
        """
        log.debug("Starting sorting data: making the file")

        if not hasattr(self, "dataSorted"):
            tmpFile = os.path.join(self.tmpDir, str(np.random.randint(0, 100000000)))
            mydict = mirnylib.h5dict.h5dict(tmpFile, 'w')
            data = mydict.add_empty_dataset("sortedData", (self.N,), mydtype)
            tmp = mydict.add_empty_dataset("trash", (self.N,), mydtype)
            code = dedent("""
            a = np.empty(len(chrms1), dtype = mydtype)
            mask = (chrms1 > chrms2) | ( (chrms1 == chrms2) & (cuts1 > cuts2))

            chrms2[mask],chrms1[mask] = chrms1[mask].copy(), chrms2[mask].copy()
            cuts1[mask],cuts2[mask] = cuts2[mask].copy(), cuts1[mask].copy()
            strands1[mask],strands2[mask] = strands2[mask].copy(),strands1[mask].copy()

            a["chrms1"] = chrms1
            a["pos1"] = cuts1
            a["chrms2"] = chrms2
            a["pos2"] = cuts2
            a["strands1"] = strands1
            a["strands2"] = strands2
            """)
            self.evaluate(expression=code, internalVariables=["chrms1", "chrms2", "cuts1", "cuts2", "strands1", "strands2"],
                          constants={"np":np, "mydtype":mydtype}, outVariable=("a", data))
            log.debug("Invoking sorter")
            externalMergeSort(data, tmp, sorter=mydtypeSorter, searchsorted=searchsorted)
            log.debug("Getting data back")
            sdata = mydict.get_dataset("sortedData")

            c1 = self.h5dict.get_dataset("chrms1")
            c2 = self.h5dict.get_dataset("chrms2")
            p1 = self.h5dict.get_dataset("cuts1")
            p2 = self.h5dict.get_dataset("cuts2")
            s1 = self.h5dict.get_dataset("strands1")
            s2 = self.h5dict.get_dataset("strands2")

            for start, end in self._getChunks():
                data = sdata[start:end]
                c1[start:end] = data["chrms1"]
                c2[start:end] = data["chrms2"]
                p1[start:end] = data["pos1"]
                p2[start:end] = data["pos2"]
                s1[start:end] = data["strands1"]
                s2[start:end] = data["strands2"]
            self.dataSorted = True
            del mydict
            os.remove(tmpFile)
            gc.collect()
            log.debug("Finished")



    def evaluate(self, expression, internalVariables, externalVariables={},
                 constants={"np": np},
                 outVariable="autodetect",
                 chunkSize="default"):
        """
        Still experimental class to perform evaluation of
        any expression on hdf5 datasets
        Note that out_variable should be writable by slices.

        ---If one can provide autodetect of values for internal
        variables by parsing an expression, it would be great!---

        .. note ::
            See example of usage of this class in filterRsiteStart,
            parseInputData, etc.

        .. warning ::
            Please avoid passing internal variables
            as "self.cuts1" - use "cuts1"

        .. warning ::
            You have to pass all the modules and functions (e.g. np)
            in a "constants" dictionary.

        Parameters
        ----------
        expression : str
            Mathematical expression, single or multi line
        internal_variables : list of str
            List of variables ("chrms1", etc.), used in the expression
        external_variables : dict , optional
            Dict of {str:array}, where str indicates name of the variable,
            and array - value of the variable.
        constants : dict, optional
            Dictionary of constants to be used in the evaluation.
            Because evaluation happens without namespace,
            you should include numpy here if you use it (included by default)
        out_variable : str or tuple or None, optional
            Variable to output the data. Either internal variable, or tuple
            (name,value), where value is an array
        """
        if isinstance(internalVariables, six.string_types):
            internalVariables = [internalVariables]

        # detecting output variable automatically
        if isinstance(outVariable, six.string_types) and (outVariable == "autodetect"):
            outVariable = expression.split("\n")[-1].split("=")[0].strip()
            if outVariable not in self.vectors:
                outVariable = (outVariable, "ToDefine")

        code = compile(expression, '<string>', 'exec')
            # compile because we're launching it many times

        for start, end in self._getChunks(chunkSize):

            variables = copy(constants)
            variables["start"] = start
            variables["end"] = end
            # dictionary to pass to the evaluator.
            # It's safer than to use the default locals()

            for name in internalVariables:
                variables[name] = self._getVector(name, start, end)

            for name, variable in list(externalVariables.items()):
                variables[name] = variable[start:end]

            # actually execute the code in our own namespace
            exec(code, variables)

            # autodetecting output dtype on the first run if not specified
            if outVariable[1] == "ToDefine":
                dtype = variables[outVariable[0]].dtype
                outVariable = (outVariable[0], np.zeros(self.N, dtype))

            if isinstance(outVariable, six.string_types):
                self.h5dict.get_dataset(outVariable)[start:end] = variables[outVariable]
            elif len(outVariable) == 2:
                outVariable[1][start:end] = variables[outVariable[0]]
            elif outVariable is None:
                pass
            else:
                raise ValueError("Please provide str or (str,value)"
                                 " for out variable")

        if type(outVariable) == tuple:
            return outVariable[1]

    def merge(self, filenames):
        """combines data from multiple datasets

        Parameters
        ----------
            filenames : list of strings
                List of folders to merge to current working folder
        """
        log.debug("Starting merge; number of datasets = {0}".format(len(filenames)))
        if self.filename in filenames:
            raise Exception("----> Cannot merge folder into itself! "
                                "Create a new folder")
        for filename in filenames:
            if not os.path.exists(filename):
                raise IOError("\nCannot open file: %s" % filename)
        log.debug("Getting h5dicts")
        h5dicts = [mirnylib.h5dict.h5dict(i, mode='r') for i in filenames]

        if all(["metadata" in i for i in h5dicts]):
            try:
                metadatas = [mydict["metadata"] for mydict in h5dicts]
            except:
                print("Failed load metadata")
            else:
                # print metadatas
                newMetadata = metadatas.pop()
                for oldData in metadatas:
                    for key, value in list(oldData.items()):
                        if (key in newMetadata):
                            try:
                                newMetadata[key] += value
                            except:
                                print("Values {0} and {1} for key {2} cannot be added".format(metadatas[key], value, key))
                                warnings.warn("Cannot add metadatas")
                        else:
                            warnings.warn("key {0} not found in some files".format(key))
                self.metadata = newMetadata
                self.h5dict["metadata"] = self.metadata
        log.debug("Calculating final length")
        self.N = sum([len(i.get_dataset("strands1")) for i in h5dicts])
        log.debug("Final length equals: {0}".format(self.N))

        for name in list(self.vectors.keys()):
            log.debug("Processing vector {0}".format(name))
            if name in self.h5dict:
                del self.h5dict[name]
            self.h5dict.add_empty_dataset(name, (self.N,), self.vectors[name])
            dset = self.h5dict.get_dataset(name)
            position = 0
            for mydict in h5dicts:
                cur = mydict[name]
                dset[position:position + len(cur)] = cur
                position += len(cur)
            self.h5dict.flush()
            time.sleep(0.2)  # allow buffers to flush
        log.debug("sorting data")
        self._sortData()
        log.debug("Finished merge")


    def parseInputData(self, dictLike, zeroBaseChrom=True,
                        **kwargs):
        """
        __NOT optimized for large datasets__
        (use chunking as suggested in pipeline2015)
        Inputs data from a dictionary-like object,
        containing coordinates of the reads.
        Performs filtering of the reads.

        A good example of a dict-like object is a numpy.savez

        .. warning::
            Restriction fragments MUST be specified
            exactly as in the Genome class.

        .. warning::
            Strand information is needed for proper scaling
            calculations, but will be imitated if not provided

        Parameters
        ----------
        dictLike : dict or dictLike object, or string with h5dict filename
            Input reads

        dictLike["chrms1,2"] : array-like
            Chromosomes of 2 sides of the read
        dictLike["cuts1,2"] :  array-like
            Exact position of cuts
        dictLike["strands1,2"], essential : array-like
            Direction of the read
        dictLike["rsites1,2"], optional : array-like
            Position of rsite to which the read is pointing
        dictLike["uprsites1,2"] , optional : array-like
            rsite upstream (larger genomic coordinate) of the cut position
        dictLike["downrsites1,2"] , optional : array-like
            rsite downstream (smaller genomic coordinate) of the cut position

        zeroBaseChrom : bool , optional
            Use zero-base chromosome counting if True, one-base if False
        enzymeToFillRsites : None or str, optional if rsites are specified
            Enzyme name to use with Bio.restriction
        keepSameFragment : bool, optional
            Do not remove same fragment reads
        noFiltering : bool, optional
            If True then no filters are applied to the data. False by default.
            Overrides removeSS. Experimental, do not use if you are not sure.
        keepSingleSided : bool, optional
            Do not remove SS reads
        keepReadsMolecules : bool, optional
            Do not remove --> <-- reads less than size selection

        """



        if isinstance(dictLike, six.string_types):
            if not os.path.exists(dictLike):
                raise IOError("File not found: %s" % dictLike)
            print("     loading data from file %s (assuming h5dict)" % dictLike)
            fname = dictLike
            dictLike = mirnylib.h5dict.h5dict(dictLike, 'r')  # attempting to open h5dict
            readCountFIle = os.path.join(os.path.split(fname)[0], "read_counts")
            if not os.path.exists(readCountFIle):
                print("raw read count file {0} does not exist")
            try:
                num = int(os.path.split(fname)[1][5:-5])
                readNums = pickle.load(open(readCountFIle))
                readNum = readNums[num - 1]
                self.metadata["000_RawReads"] = readNum
                print("Found raw read count:", readNum)
            except:
                print("Count not find raw read count")

        "---Filling in chromosomes and positions - mandatory objects---"
        a = dictLike["chrms1"]
        self.trackLen = len(a)

        if zeroBaseChrom == True:
            self.chrms1 = a
            self.chrms2 = dictLike["chrms2"]
        else:
            self.chrms1 = a - 1
            self.chrms2 = dictLike["chrms2"] - 1
        self.N = len(self.chrms1)

        self.metadata["010_MappedSide1"] = (self.chrms1 >= 0).sum()
        self.metadata["020_MappedSide2"] = (self.chrms2 >= 0).sum()

        del a

        self.cuts1 = dictLike['cuts1']
        self.cuts2 = dictLike['cuts2']


        if not (("strands1" in list(dictLike.keys())) and
                ("strands2" in list(dictLike.keys()))):
            warnings.warn("No strand information provided,"
                          " assigning random strands.")
            t = np.random.randint(0, 2, self.trackLen)
            self.strands1 = t
            self.strands2 = 1 - t
            del t
            noStrand = True
        else:
            self.strands1 = dictLike["strands1"]
            self.strands2 = dictLike["strands2"]
            noStrand = False  # strand information filled in

        self.metadata["100_TotalReads"] = self.trackLen

        try:
            dictLike["misc"]["genome"]["idx2label"]
            self.updateGenome(self.genome, oldGenome=dictLike["misc"]["genome"]["idx2label"], putMetadata=True)

        except KeyError:
            assumedGenome = Genome(self.genome.genomePath)
            self.updateGenome(self.genome, oldGenome=assumedGenome, putMetadata=True)
            warnings.warn("\n Genome not found in mapped data. \n"
            "Assuming genome comes from the same folder with all chromosomes")

        self.metadata["152_removedUnusedChromosomes"] = self.trackLen - self.N
        self.metadata["150_ReadsWithoutUnusedChromosomes"] = self.N

        # Discard single-sided reads
        DSmask = (self.chrms1 >= 0) * (self.chrms2 >= 0)
        self.metadata["200_totalDSReads"] = DSmask.sum()
        self.metadata["201_DS+SS"] = len(DSmask)
        self.metadata["202_SSReadsRemoved"] = len(DSmask) - DSmask.sum()
        if not kwargs.get("keepSingleSided", False):
            print('filtering SS reads')
            mask = DSmask
        else:
            print('keeping SS reads')
            mask = np.ones(np.shape(DSmask)) > 0

        # Discard dangling ends and self-circles
        if not kwargs.get("keepSameFragment", False):
            sameFragMask = self.evaluate("a = (rfragAbsIdxs1 == rfragAbsIdxs2)", ["rfragAbsIdxs1", "rfragAbsIdxs2"]) * DSmask
            cutDifs = self.cuts2[sameFragMask] > self.cuts1[sameFragMask]
            s1 = self.strands1[sameFragMask]
            s2 = self.strands2[sameFragMask]
            SSDE = (s1 != s2)
            SS = SSDE * (cutDifs == s2)
            SS_N = SS.sum()
            SSDE_N = SSDE.sum()
            sameFrag_N = sameFragMask.sum()
            self.metadata["210_sameFragmentReadsRemoved"] = sameFrag_N
            self.metadata["212_Self-Circles"] = SS_N
            self.metadata["214_DandlingEnds"] = SSDE_N - SS_N
            self.metadata["216_error"] = sameFrag_N - SSDE_N
            print("Removing same-fragment reads")
            mask  *= (-sameFragMask)
            del DSmask, sameFragMask
        else:
            print("Keeping same-fragment reads")
        noSameFrag = mask.sum()

        # Discard reads longer than size-selection
        if not kwargs.get("keepReadsMolecules", False):
            if noStrand == True:
                # Can't tell if reads point to each other.
                dist = self.evaluate("a = np.abs(cuts1 - cuts2)",
                                     ["cuts1", "cuts2"])
            else:
                # distance between sites facing each other
                dist = self.evaluate("a = numexpr.evaluate('- cuts1 * (2 * strands1 -1) - "
                                     "cuts2 * (2 * strands2 - 1)')",
                                     ["cuts1", "cuts2", "strands1", "strands2"],
                                     constants={"numexpr":numexpr})

            readsMolecules = self.evaluate(
               "a = numexpr.evaluate('(chrms1 == chrms2)&(strands1 != strands2) &  (dist >=0) &"
               " (dist <= maximumMoleculeLength)')",
               internalVariables=["chrms1", "chrms2", "strands1", "strands2"],
               externalVariables={"dist": dist},
               constants={"maximumMoleculeLength": self.maximumMoleculeLength, "numexpr":numexpr})
            mask *= (readsMolecules == False)
            print('removing extraDEs, when they exceed maximumMoleculeLength')
            extraDE = mask.sum()
            self.metadata["220_extraDandlingEndsRemoved"] = -extraDE + noSameFrag
            del dist
            del readsMolecules
        else:
            print('keeping --> <-- reads even if they exceed maximumMoleculeLength')

        # Summary & Tests
        if mask.sum() == 0:
            raise Exception('No reads left after filtering. Please, check the input data')
        if not kwargs.get('noFiltering', False):
            self.maskFilter(mask)
        self.metadata["300_ValidPairs"] = self.N
        print('300_validPairs: ', self.N)
        del dictLike


    def printMetadata(self, saveTo=None):
        self._dumpMetadata()
        for i in sorted(self.metadata):
            if i[2] != "0":
                print("\t\t", end=' ')
            elif i[1] != "0":
                print("\t", end=' ')
            print(i, self.metadata[i])
        if saveTo != None:
            with open(saveTo, 'w') as myfile:
                for i in sorted(self.metadata):
                    if i[2] != "0":
                        myfile.write("\t\t")
                    elif i[1] != "0":
                        myfile.write("\t")
                    myfile.write(str(i))
                    myfile.write(":   ")
                    myfile.write(str(self.metadata[i]))
                    myfile.write("\n")

    def updateGenome(self, newGenome, oldGenome="current", putMetadata=False):
        """
        __partially optimized for large datasets__
        Updates dataset to a new genome, with a fewer number of chromosomes.
        Use it to delete chromosomes.
        By default, removes all DS reads with that chromosomes.

        Parameters
        ----------
        newGenome : Genome object
            Genome to replace the old genome, with fewer chromosomes
        removeSSreads : "trans"(default), "all" or "none"
            "trans": remove all reads from deleted chromosomes,
            ignore the rest.
            "all": remove all SS reads from all chromosomes
            "None": mark all trans reads as SS reads
        putMetadata : bool (optional)
            Writes metadata for M and Y reads

        oldGenome : Genome object or idx2label of old genome, optional

        """

        assert isinstance(newGenome, Genome)
        newN = newGenome.chrmCount
        if oldGenome == "current":
            oldGenome = self.genome
        upgrade = newGenome.upgradeMatrix(oldGenome)
        if isinstance(oldGenome, Genome):
            if (oldGenome.hasEnzyme()==True) and (newGenome.hasEnzyme()==False):
                newGenome.setEnzyme(oldGenome.enzymeName)
            oldGenome = oldGenome.idx2label
        oldN = len(list(oldGenome.keys()))
        label2idx = dict(list(zip(list(oldGenome.values()), list(oldGenome.keys()))))
        chrms1 = np.array(self.chrms1, int)
        chrms2 = np.array(self.chrms2, int)
        SS = (chrms1 < 0) + (chrms2 < 0)
        metadata = {}
        if ("M" in label2idx) or ("MT" in label2idx):
            if ("M" in label2idx):
                Midx = label2idx["M"]
            if ("MT" in label2idx):
                Midx = label2idx["MT"]
            M1 = chrms1 == Midx
            M2 = chrms2 == Midx
            mToM = (M1 * M2).sum()
            mToAny = (M1 + M2).sum()
            mToSS = ((M1 + M2) * SS).sum()
            metadata["102_mappedSide1"] = (chrms1 >= 0).sum()
            metadata["104_mappedSide2"] = (chrms2 >= 0).sum()

            metadata["112_M-to-M_reads"] = mToM
            metadata["114_M-to-Any_reads"] = mToAny
            metadata["116_M-to-SS_reads"] = mToSS
            metadata["118_M-to-DS_reads"] = mToAny - mToSS

        if "Y" in label2idx:
            Yidx = label2idx["Y"]
            Y1 = chrms1 == Yidx
            Y2 = chrms2 == Yidx
            yToY = (Y1 * Y2).sum()
            yToAny = (Y1 + Y2).sum()
            yToSS = ((Y1 + Y2) * SS).sum()

            metadata["122_Y-to-Y_reads"] = yToY
            metadata["124_Y-to-Any_reads"] = yToAny
            metadata["126_Y-to-SS_reads"] = yToSS
            metadata["128_Y-to-DS_reads"] = yToAny - yToSS

        if putMetadata:
            self.metadata.update(metadata)

        if upgrade is not None:
            upgrade[upgrade == -1] = 9999  # to tell old SS reads from new SS reads

            chrms1 = upgrade[chrms1]
            self.chrms1 = chrms1
            del chrms1

            chrms2 = upgrade[chrms2]
            self.chrms2 = chrms2

        "Keeping only DS reads"
        mask = ((self.chrms1 < newN) * (self.chrms2 < newN))
        self.genome = newGenome
        self.maskFilter(mask)

    def buildAllHeatmap(self, resolution, countDiagonalReads="Once",
        useWeights=False):
        """
        __optimized for large datasets__
        Creates an all-by-all heatmap in accordance with mapping
        provided by 'genome' class

        Parameters
        ----------
        resolution : int or str
            Resolution of a heatmap. May be an int or 'fragment' for
            restriction fragment resolution.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        useWeights : bool
            If True, then take weights from 'weights' variable. False by default.
        """
        for start, end in self._getChunks(30000000):
            if type(resolution) == int:
                # 8 bytes per record + heatmap
                self.genome.setResolution(resolution)
                numBins = self.genome.numBins

                label = self.genome.chrmStartsBinCont[self._getVector("chrms1", start, end)]
                label = np.asarray(label, dtype="int64")
                label += (self._getVector("mids1", start, end) // resolution).astype(np.int64)
                label *= numBins
                label += self.genome.chrmStartsBinCont[self._getVector("chrms2", start, end)]
                label += (self._getVector("mids2", start, end) // resolution).astype(np.int64)

            elif resolution == 'fragment':
                numBins = self.genome.numRfrags
                label = self._getVector("rfragAbsIdxs1", start, end)
                label *= numBins
                label += self._getVector("rfragAbsIdxs2", start, end)
            else:
                raise Exception('Unknown value for resolution: {0}'.format(
                    resolution))

            if useWeights:
                if 'weights' not in self.vectors:
                    raise Exception('Set read weights first!')
                counts = np.bincount(label, weights=self.fragmentWeights, minlength=numBins ** 2)
            else:
                counts = np.bincount(label, minlength=numBins ** 2)
            if len(counts) > numBins ** 2:
                raise Exception("\nheatmap exceed length of the genome!!!"
                                    " Check genome")

            counts.shape = (numBins, numBins)
            try:
                heatmap += counts  # @UndefinedVariable
            except:
                heatmap = counts

        for i in range(len(heatmap)):
            heatmap[i, i:] += heatmap[i:, i]
            heatmap[i:, i] = heatmap[i, i:]
        if countDiagonalReads.lower() == "once":
            diag = np.diag(heatmap)
            fillDiagonal(heatmap, diag / 2)
        elif countDiagonalReads.lower() == "twice":
            pass
        else:
            raise ValueError("Bad value for countDiagonalReads")
        return heatmap



    def getFragmentHeatmap(self, absFragIdx1=0, absFragIdx2=None):
        """returns fragment heatmap from absFragIdx1 to absFragIdx2 not including the end
        Sort of works, but is still buggy"""
        c1 = self.h5dict.get_dataset("chrms1")
        p1 = self.h5dict.get_dataset("cuts1")


        rsiteId1 = self.rsites[absFragIdx1 - 1] + 6
        rsiteId2 = self.rsites[absFragIdx2 - 1] - 6
        if absFragIdx1 == 0:
            low = 0
        else:
            clow, plow = rsiteId1 // self.fragIDmult, rsiteId1 % self.fragIDmult
            low = h5dictBinarySearch(c1, p1, (clow, plow + 5), "left") + 1

        if absFragIdx2 == None:
            absFragIdx2 = len(self.rFragIDs)
            high = self.N
        else:
            chigh, phigh = rsiteId2 // self.fragIDmult, rsiteId2 % self.fragIDmult
            high = h5dictBinarySearch(c1, p1, (chigh, phigh - 5), "right") - 1

        setExceptionHook()
        M = absFragIdx2 - absFragIdx1
        if M > 10000:
            warnings.warn("Returned heatmap is more than 10000 long, make sure you have enough memory!")
        elif M > 100000:
            raise ValueError("Cannot build a heatmap more than 100000 long ")
        ind = np.array(self._getVector("rfragAbsIdxs1", low, high)) - absFragIdx1
        assert ind.min() >= 0
        assert ind.max() < M
        ind2 = np.array(self._getVector("rfragAbsIdxs2", low, high)) - absFragIdx1
        mask = (ind2 >= 0) * (ind2 < M)
        ind = ind[mask]
        ind2 = ind2[mask]
        assert (ind < ind2).sum() == len(ind)

        ind2d = ind + M * ind2
        matrix = np.bincount(ind2d, minlength=M * M)
        matrix.shape = ((M, M))
        return matrix + matrix.T





    def fragmentFilter(self, fragments):
        """
        __optimized for large datasets__
        keeps only reads that originate from fragments in 'fragments'
        variable, for DS - on both sides

        Parameters
        ----------
        fragments : numpy.array of fragment IDs or bools
            List of fragments to keep, or their indexes in self.rFragIDs
        """
        if fragments.dtype == np.bool:
            fragments = self.rFragIDs[fragments]
        m1 = arrayInArray(self._getSliceableVector("fragids1"), fragments, chunkSize=self.chunksize)
        m2 = arrayInArray(self._getSliceableVector("fragids2"), fragments, chunkSize=self.chunksize)
        mask = np.logical_and(m1, m2)
        self.maskFilter(mask)

    def maskFilter(self, mask):
        """
        __optimized for large datasets__
        keeps only reads designated by mask

        Parameters
        ----------
        mask : array of bools
            Indexes of reads to keep
        """
        # Uses 16 bytes per read
        for i in list(self.rfragIDDict.keys()):
            del self.rfragIDDict[i]

        length = 0
        ms = mask.sum()
        assert mask.dtype == np.bool
        self.N = ms
        if ms == 0:
            self.N = 0
            return

        for name in self.vectors:
            data = self._getData(name)
            ld = len(data)
            if length == 0:
                length = ld
            else:
                if ld != length:
                    self.delete()
            newdata = data[mask]
            del data
            self._setData(name, newdata)
            del newdata
        del mask



    def filterExtreme(self, cutH=0.005, cutL=0):
        """
        __optimized for large datasets__
        removes fragments with most and/or least # counts

        Parameters
        ----------
        cutH : float, 0<=cutH < 1, optional
            Fraction of the most-counts fragments to be removed
        cutL : float, 0<=cutL<1, optional
            Fraction of the least-counts fragments to be removed
        """
        print("----->Extreme fragments filter: remove top %lf, "\
        "bottom %lf fragments" % (cutH, cutL))

        s = self.fragmentSum()
        ss = np.sort(s)

        valueL, valueH = np.percentile(ss[ss > 0], [100. * cutL, 100 * (1. - cutH)])
        news = (s >= valueL) * (s <= valueH)
        N1 = self.N
        self.fragmentFilter(self.rFragIDs[news])
        self.metadata["350_removedFromExtremeFragments"] = N1 - self.N
        self._dumpMetadata()

        print("     #Top fragments are: ", ss[-10:])
        print("     # Cutoff for low # counts is (counts): ", valueL, end=' ')
        print("; cutoff for large # counts is: ", valueH, "\n")

    def filterLarge(self, cutlarge=100000, cutsmall=100):
        """
        __optimized for large datasets__
        removes very large and small fragments

        Parameters
        ----------
        cutlarge : int
            remove fragments larger than it
        cutsmall : int
            remove fragments smaller than it
        """
        print("----->Small/large fragments filter: keep strictly less"\
        "than %d,strictly more than %d bp" % (cutlarge, cutsmall))
        p = (self.rFragLens < (cutlarge)) * (self.rFragLens > cutsmall)
        N1 = self.N
        self.fragmentFilter(self.rFragIDs[p])
        N2 = self.N
        self.metadata["340_removedLargeSmallFragments"] = N1 - N2
        self._dumpMetadata()

    def filterRsiteStart(self, offset=5):
        """
        __optimized for large datasets__
        Removes reads that start within x bp near rsite

        Parameters
        ----------

        offset : int
            Number of bp to exclude next to rsite, not including offset

        """

        # TODO:(MI) fix this so that it agrees with the definition.

        print("----->Semi-dangling end filter: remove guys who start %d"\
        " bp near the rsite" % offset)


        expression = 'mask = numexpr.evaluate("(abs(dists1 - fraglens1) >= offset) & '\
        '((abs(dists2 - fraglens2) >= offset))")'


        mask = self.evaluate(expression,
                             internalVariables=["dists1", "fraglens1",
                                                "dists2", "fraglens2"],
                             constants={"offset": offset, "np": np, "numexpr":numexpr},
                             outVariable=("mask", np.zeros(self.N, bool)))
        self.metadata["310_startNearRsiteRemoved"] = len(mask) - mask.sum()
        self.maskFilter(mask)

    def filterDuplicates(self, mode="auto", tmpDir="default", chunkSize=100000000):
        """
        __optimized for large datasets__
        removes duplicate molecules"""

        # Uses a lot!
        print("----->Filtering duplicates in DS reads: ")

        if mode == "auto":
            if self.N > 400000000:
                mode = "hdd"
            else:
                mode = "ram"

        if tmpDir == "default":
            tmpDir = self.tmpDir
        # an array to determine unique rows. Eats 1 byte per DS record

        if mode == "ram":
            log.debug("Filtering duplicates in RAM")
            dups = np.zeros((self.N, 2), dtype="int64", order="C")
            dups[:, 0] = self.chrms1
            dups[:, 0] *= self.fragIDmult
            dups[:, 0] += self.cuts1
            dups[:, 1] = self.chrms2
            dups[:, 1] *= self.fragIDmult
            dups[:, 1] += self.cuts2
            dups.sort(axis=1)
            dups.shape = (self.N * 2)
            strings = dups.view("|S16")
                # Converting two indices to a single string to run unique
            uids = uniqueIndex(strings)
            del strings, dups
            stay = np.zeros(self.N, bool)
            stay[uids] = True  # indexes of unique DS elements
            del uids

        elif mode == "hdd":
            tmpFile = os.path.join(tmpDir, str(np.random.randint(0, 100000000)))
            a = mirnylib.h5dict.h5dict(tmpFile)
            a.add_empty_dataset("duplicates", (self.N,), dtype="|S24")
            a.add_empty_dataset("temp", (self.N,), dtype="|S24")
            dset = a.get_dataset("duplicates")
            tempdset = a.get_dataset("temp")
            code = dedent("""
            tmp = np.array(chrms1, dtype=np.int64) * fragIDmult + cuts1
            tmp2 = np.array(chrms2, dtype=np.int64) * fragIDmult + cuts2
            newarray = np.zeros((len(tmp),3), dtype = np.int64)
            newarray[:,0] = tmp
            newarray[:,1] = tmp2
            newarray[:,:2].sort(axis=1)
            newarray[:,2] = np.arange(start, end, dtype=np.int64)
            newarray.shape = (3*len(tmp))
            a = np.array(newarray.view("|S24"))
            assert len(a) == len(chrms1)
            """)
            self.evaluate(code, ["chrms1", "cuts1", "chrms2", "cuts2"],
                          constants={"np":np, "fragIDmult":self.fragIDmult},
                          outVariable=("a", dset))
            stay = np.zeros(self.N, bool)
            numutils.externalMergeSort(dset, tempdset, chunkSize=chunkSize)
            bins = list(range(0, self.N - 1000, self.chunksize)) + [self.N - 1]
            for start, end in zip(bins[:-1], bins[1:]):
                curset = dset[start:end + 1]
                curset = curset.view(dtype=np.int64)
                curset.shape = (len(curset) // 3, 3)
                unique = (curset[:-1, 0] != curset[1:, 0]) + (curset[:-1, 1] != curset[1:, 1])
                stay[curset[:, 2][unique]] = True
                if end == self.N - 1:
                    stay[curset[-1, 2]] = True
            del a
            del tmpFile

        self.metadata["320_duplicatesRemoved"] = len(stay) - stay.sum()
        self.maskFilter(stay)

    def filterSingleSided(self):
        # Discard single-sided reads
        DSmask = (self.chrms1 >= 0) * (self.chrms2 >= 0)
        self.metadata["200_totalDSReads"] = DSmask.sum()
        self.metadata["201_DS+SS"] = len(DSmask)
        self.metadata["202_SSReadsRemoved"] = len(DSmask) - DSmask.sum()
        print('filtering SS reads')
        self.maskFilter(DSmask)

    def filterByCisToTotal(self, cutH=0.0, cutL=0.01):
        """
        __NOT optimized for large datasets__
        Remove fragments with too low or too high cis-to-total ratio.
        Parameters
        ----------
        cutH : float, 0<=cutH < 1, optional
            Fraction of the fragments with largest cis-to-total ratio
            to be removed.
        cutL : float, 0<=cutL<1, optional
            Fraction of the fragments with lowest cis-to-total ratio
            to be removed.
        """


        concRfragAbsIdxs = np.r_[self.rfragAbsIdxs1, self.rfragAbsIdxs2]
        concCis = np.r_[self.chrms1 == self.chrms2, self.chrms1 == self.chrms2]

        cis = np.bincount(concRfragAbsIdxs[concCis])
        total = np.bincount(concRfragAbsIdxs)
        cistototal = np.nan_to_num(cis / total.astype('float'))
        numEmptyFrags = (cistototal == 0).sum()

        cutLFrags = int(np.ceil((len(cistototal) - numEmptyFrags) * cutL))
        cutHFrags = int(np.ceil((len(cistototal) - numEmptyFrags) * cutH))

        sortedCistotot = np.sort(cistototal)

        lCutoff = sortedCistotot[cutLFrags + numEmptyFrags]
        hCutoff = sortedCistotot[len(cistototal) - 1 - cutHFrags]

        fragsToFilter = np.where((cistototal < lCutoff) + (cistototal > hCutoff))[0]
        print(('Keep fragments with cis-to-total ratio in range ({0},{1}), '
               'discard {2} fragments').format(lCutoff, hCutoff, cutLFrags + cutHFrags))

        mask = (arrayInArray(self.rfragAbsIdxs1, fragsToFilter) +
                arrayInArray(self.rfragAbsIdxs2, fragsToFilter))

        self.metadata["330_removedByCisToTotal"] = mask.sum()

        self.maskFilter(-mask)

    def filterTooClose(self, minRsitesDist=2):
        """
        __NOT optimized for large datasets__
        Remove fragment pairs separated by less then `minRsitesDist`
        restriction sites within the same chromosome.
        """
        mask = (np.abs(self.rfragAbsIdxs1 - self.rfragAbsIdxs2) < minRsitesDist) * (self.chrms1 == self.chrms2)
        self.metadata["360_closeFragmentsRemoved"] = mask.sum()
        print('360_closeFragmentsRemoved: ', mask.sum())
        self.maskFilter(-mask)

    def filterOrientation(self):
        "__NOT optimized for large datasets__"
        # Keep only --> -->  or <-- <-- pairs, discard --> <-- and <-- -->
        mask = (self.strands1 == self.strands2)
        self.metadata["370_differentOrientationReadsRemoved"] = mask.sum()
        print('370_differentOrientationReadsRemoved: ', mask.sum())
        self.maskFilter(-mask)

    def writeFilteringStats(self):
        self.metadata["400_readsAfterFiltering"] = self.N
        sameChrom = self.chrms1 == self.chrms2
        self.metadata["401_cisReads"] = sameChrom.sum()
        self.metadata["402_transReads"] = self.N - sameChrom.sum()
        self._dumpMetadata()

    def fragmentSum(self, fragments=None, strands="both", useWeights=False):

        """
        __optimized for large datasets__
        returns sum of all counts for a set or subset of fragments


        Parameters
        ----------
        fragments : list of fragment IDs, optional
            Use only this fragments. By default all fragments are used
        strands : 1,2 or "both" (default)
            Use only first or second side of the read
            (first has SS, second - doesn't)
        useWeights : bool, optional
            If set to True, will give a fragment sum with weights adjusted for iterative correction.
        """
        # Uses 0 bytes per read

        if fragments is None:
            fragments = self.rFragIDs

        if not useWeights:
            f1 = chunkedBincount(self._getSliceableVector("rfragAbsIdxs1"), minlength=len(self.rFragIDs))
            f2 = chunkedBincount(self._getSliceableVector("rfragAbsIdxs2"), minlength=len(self.rFragIDs))
            if strands == "both":
                return f1 + f2
            if strands == 1:
                return f1
            if strands == 2:
                return f2
        else:
            if strands == "both":
                pass1 = 1. / self.fragmentWeights[arraySearch(self.rFragIDs, self.fragids1)]
                pass1 /= self.fragmentWeights[arraySearch(self.rFragIDs, self.fragids2)]
                return arraySumByArray(self.fragids1, fragments, pass1) + arraySumByArray(self.fragids2, fragments, pass1)
            else:
                raise NotImplementedError("Sorry")

    def iterativeCorrectionFromMax(self, minimumCount=50, precision=0.01):
        "TODO: rewrite this to account for a new fragment model"
        biases = np.ones(len(self.rFragMids), dtype=np.double)
        self.fragmentWeights = 1. * self.fragmentSum()
        self.fragmentFilter(self.fragmentWeights > minimumCount)
        self.fragmentWeights = 1. * self.fragmentSum()

        while True:
            newSum = 1. * self.fragmentSum(useWeights=True)
            biases *= newSum / newSum.mean()
            maxDev = np.max(np.abs(newSum - newSum.mean())) / newSum.mean()
            print(maxDev)

            self.fragmentWeights *= (newSum / newSum.mean())
            if maxDev < precision:
                return biases

    def printStats(self):
        self.printMetadata()

    def save(self, filename):
        "Saves dataset to filename, does not change the working file."
        if self.filename == filename:
            raise Exception("Cannot save to the working file")
        newh5dict = mirnylib.h5dict.h5dict(filename, mode='w')
        for name in list(self.vectors.keys()):
            newh5dict[name] = self.h5dict[name]
        newh5dict["metadata"] = self.metadata
        print("----> Data saved to file %s" % (filename,))

    def load(self, filename, buildFragments="deprecated"):
        "Loads dataset from file to working file; check for inconsistency"
        otherh5dict = mirnylib.h5dict.h5dict(filename, 'r')
        if "metadata" in otherh5dict:
            try:
                self.metadata = otherh5dict["metadata"]
            except:
                warnings.warn("Metadata not found!!!")
        else:
            print(list(otherh5dict.keys()))
            warnings.warn("Metadata not found!!!")

        length = 0
        for name in self.vectors:
            data = otherh5dict[name]
            ld = len(data)
            if length == 0:
                length = ld
            else:
                if ld != length:
                    print(("---->!!!!!File %s contains inconsistend data<----" %
                          filename))
                    self.exitProgram("----> Sorry...")

            self._setData(name, data)
        print("---->Loaded data from file %s, contains %d reads" % (
            filename, length))
        self.N = length

        self._checkConsistency()

    def saveHeatmap(self, filename, resolution=1000000,
                    countDiagonalReads="Once",
                    useWeights=False,
                    useFragmentOverlap=False, maxBinSpawn=10):
        """
        Saves heatmap to filename at given resolution.
        For small genomes where number of fragments per bin is small,
        please set useFragmentOverlap to True.
        This will assign each fragment to all bins over which the fragment
        spawns.

        Parameters
        ----------
        filename : str
            Filename of the output h5dict
        resolution : int or str
            Resolution of a heatmap. May be an int or 'fragment' for
            restriction fragment resolution.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        useWeights : bool
            If True, then take weights from 'weights' variable. False by default.
            If using iterativeCorrectionFromMax (fragment-level IC), use weights.
        useFragmentOverlap : bool (optional)
            Set this to true if you have few fragments per bin (bin size <20kb for HindIII)
            It will consume more RAM and be slower.
        """

        try:
            os.remove(filename)
        except:
            pass

        tosave = mirnylib.h5dict.h5dict(path=filename, mode="w")
        if not useFragmentOverlap:
            heatmap = self.buildAllHeatmap(resolution, countDiagonalReads, useWeights)
        else:
            heatmap = self.buildHeatmapWithOverlapCpp(resolution, countDiagonalReads, maxBinSpawn)

        tosave["heatmap"] = heatmap
        del heatmap
        if resolution != 'fragment':
            chromosomeStarts = np.array(self.genome.chrmStartsBinCont)
            numBins = self.genome.numBins
        else:
            chromosomeStarts = np.array(self.genome.chrmStartsRfragCont)
            numBins = self.genome.numRfrags
        tosave["resolution"] = resolution
        tosave["genomeBinNum"] = numBins
        tosave["genomeIdxToLabel"] = self.genome.idx2label
        tosave["chromosomeStarts"] = chromosomeStarts
        print("----> Heatmap saved to '{0}' at {1} resolution".format(
            filename, resolution))

    def saveByChromosomeHeatmap(self, filename, resolution=10000,
                                includeTrans=True,
                                countDiagonalReads="Once"):
        """
        Saves chromosome by chromosome heatmaps to h5dict.
        This method is not as memory demanding as saving allxall heatmap.

        Keys of the h5dict are of the format ["1 14"],
        where chromosomes are zero-based,
        and there is one space between numbers.

        .. warning :: Chromosome numbers are always zero-based.
        Only "chr3" labels are one-based in this package.

        Parameters
        ----------

        filename : str
            Filename of the h5dict with the output
        resolution : int
            Resolution to save heatmaps
        includeTrans : bool, optional
            Build inter-chromosomal heatmaps (default: False)
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin

        """
        if countDiagonalReads.lower() not in ["once", "twice"]:
            raise ValueError("Bad value for countDiagonalReads")
        self.genome.setResolution(resolution)

        mydict = mirnylib.h5dict.h5dict(filename)

        for chromosome in range(self.genome.chrmCount):
            c1 = self.h5dict.get_dataset("chrms1")
            p1 = self.h5dict.get_dataset("cuts1")
            low = h5dictBinarySearch(c1, p1, (chromosome, -1), "left")
            high = h5dictBinarySearch(c1, p1, (chromosome, 999999999), "right")

            chr1 = self._getVector("chrms1", low, high)
            chr2 = self._getVector("chrms2", low, high)
            pos1 = np.array(self._getVector("mids1", low, high) // resolution, dtype=np.int32)
            pos2 = np.array(self._getVector("mids2", low, high) // resolution, dtype=np.int32)

            assert (chr1 == chromosome).all()  # getting sure that bincount worked

            args = np.argsort(chr2)
            chr2 = chr2[args]
            pos1 = pos1[args]
            pos2 = pos2[args]

            for chrom2 in range(chromosome, self.genome.chrmCount):
                if (includeTrans == False) and (chrom2 != chromosome):
                    continue
                start = np.searchsorted(chr2, chrom2, "left")
                end = np.searchsorted(chr2, chrom2, "right")
                cur1 = pos1[start:end]
                cur2 = pos2[start:end]
                label = np.array(cur1, "int64")
                label *= self.genome.chrmLensBin[chrom2]
                label += cur2
                maxLabel = self.genome.chrmLensBin[chromosome] * \
                           self.genome.chrmLensBin[chrom2]
                counts = np.bincount(label, minlength=maxLabel)
                mymap = counts.reshape((self.genome.chrmLensBin[chromosome], -1))
                if chromosome == chrom2:
                    mymap = mymap + mymap.T
                    if countDiagonalReads.lower() == "once":
                        fillDiagonal(mymap, np.diag(mymap).copy() / 2)
                mydict["%d %d" % (chromosome, chrom2)] = mymap
        print("----> By chromosome Heatmap saved to '{0}' at {1} resolution".format(filename, resolution))

        return




    def buildHeatmapWithOverlapCpp(self, resolution, countDiagonalReads="Twice",
        maxBinSpawn=10):
        """
        __NOT optimized for large datasets__
        Creates an all-by-all heatmap in accordance with mapping
        provided by 'genome' class

        This method assigns fragments to all bins which
        the fragment overlaps, proportionally

        Parameters
        ----------
        resolution : int or str
            Resolution of a heatmap. May be an int or 'fragment' for
            restriction fragment resolution.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        maxBinSpawn : int, optional, not more than 10
            Discard read if it spawns more than maxBinSpawn bins

        """

        if type(resolution) == int:

            # many bytes per record + heatmap
            self.genome.setResolution(resolution)
            N = self.N
            N = int(N)

            low1 = self.genome.chrmStartsBinCont[self.chrms1]
            low1 = np.asarray(low1, dtype="float32")
            low1 += (self.mids1 - self.fraglens1 / 2) / float(resolution)

            high1 = self.genome.chrmStartsBinCont[self.chrms1]
            high1 = np.asarray(high1, dtype="float32")
            high1 += (self.mids1 + self.fraglens1 / 2) / float(resolution)

            low2 = self.genome.chrmStartsBinCont[self.chrms2]
            low2 = np.asarray(low2, dtype="float32")
            low2 += (self.mids2 - self.fraglens2 / 2) / float(resolution)

            high2 = self.genome.chrmStartsBinCont[self.chrms2]
            high2 = np.asarray(high2, dtype="float32")
            high2 += (self.mids2 + self.fraglens2 / 2) / float(resolution)


            heatmap = np.zeros((self.genome.numBins, self.genome.numBins),
                               dtype="float64", order="C")
            heatmapSize = len(heatmap)  # @UnusedVariable


            from scipy import weave
            code = """
            #line 1045 "fragmentHiC.py"
            double vector1[100];
            double vector2[100];

            for (int readNum = 0;  readNum < N; readNum++)
            {
                for (int i=0; i<10; i++)
                {
                    vector1[i] = 0;
                    vector2[i] = 0;
                }

                double l1 = low1[readNum];
                double l2 = low2[readNum];
                double h1 = high1[readNum];
                double h2 = high2[readNum];


                if ((h1 - l1) > maxBinSpawn) continue;
                if ((h2 - l2) > maxBinSpawn) continue;

                int binNum1 = ceil(h1) - floor(l1);
                int binNum2 = ceil(h2) - floor(l2);
                double binLen1 = h1 - l1;
                double binLen2 = h2 - l2;

                int b1 = floor(l1);
                int b2 = floor(l2);

                if (binNum1 == 1)
                    vector1[0] = 1.;
                else
                    {
                    vector1[0] = (ceil(l1 + 0.00001) - l1) / binLen1;
                    for (int t = 1; t< binNum1 - 1; t++)
                        {vector1[t] = 1. / binLen1;}
                    vector1[binNum1 - 1] = (h1 - floor(h1)) / binLen1;
                    }

                if (binNum2 == 1) vector2[0] = 1.;

                else
                    {
                    vector2[0] = (ceil(l2 + 0.0001) - l2) / binLen2;
                    for (int t = 1; t< binNum2 - 1; t++)
                        {vector2[t] = 1. / binLen2;}
                    vector2[binNum2 - 1] = (h2 - floor(h2)) / binLen2;
                    }

                for (int i = 0; i< binNum1; i++)
                    {
                    for (int j = 0; j < binNum2; j++)
                        {
                        heatmap[(b1 + i) * heatmapSize +  b2 + j] += vector1[i] * vector2[j];
                        }
                    }
                }
        """
        weave.inline(code,
                     ['low1', "high1", "low2", "high2",
                       "N", "heatmap", "maxBinSpawn",
                      "heatmapSize",
                       ],
                     extra_compile_args=['-march=native  -O3 '],
                     support_code=r"""
                    #include <stdio.h>
                    #include <math.h>""")

        counts = heatmap
        for i in range(len(counts)):
            counts[i, i:] += counts[i:, i]
            counts[i:, i] = counts[i, i:]
            diag = np.diag(counts)
        if countDiagonalReads.lower() == "once":
            fillDiagonal(counts, diag / 2)
        elif countDiagonalReads.lower() == "twice":
            pass
        else:
            raise ValueError("Bad value for countDiagonalReads")
        return counts

    def getFragmentHeatmap(self, absFragIdx1=0, absFragIdx2=None):
        """returns fragment heatmap from absFragIdx1 to absFragIdx2 not including the end
        Sort of works, but is still buggy"""
        c1 = self.h5dict.get_dataset("chrms1")
        p1 = self.h5dict.get_dataset("cuts1")


        rsiteId1 = self.rsites[absFragIdx1 - 1] + 6
        rsiteId2 = self.rsites[absFragIdx2 - 1] - 6
        if absFragIdx1 == 0:
            low = 0
        else:
            clow, plow = rsiteId1 / self.fragIDmult, rsiteId1 % self.fragIDmult
            low = h5dictBinarySearch(c1, p1, (clow, plow + 5), "left") + 1

        if absFragIdx2 == None:
            absFragIdx2 = len(self.rFragIDs)
            high = self.N
        else:
            chigh, phigh = rsiteId2 / self.fragIDmult, rsiteId2 % self.fragIDmult
            high = h5dictBinarySearch(c1, p1, (chigh, phigh - 5), "right") - 1

        setExceptionHook()
        M = absFragIdx2 - absFragIdx1
        if M > 10000:
            warnings.warn("Returned heatmap is more than 10000 long, make sure you have enough memory!")
        elif M > 100000:
            raise ValueError("Cannot build a heatmap more than 100000 long ")
        ind = np.array(self._getVector("rfragAbsIdxs1", low, high)) - absFragIdx1
        assert ind.min() >= 0
        assert ind.max() < M
        ind2 = np.array(self._getVector("rfragAbsIdxs2", low, high)) - absFragIdx1
        mask = (ind2 >= 0) * (ind2 < M)
        ind = ind[mask]
        ind2 = ind2[mask]
        assert (ind < ind2).sum() == len(ind)

        ind2d = ind + M * ind2
        matrix = np.bincount(ind2d, minlength=M * M)
        matrix.shape = ((M, M))
        return matrix + matrix.T


    def getHiResHeatmap(self, resolution, chromosome, start=0, end=None, countDiagonalReads="Twice", maxBinSpawn=10):
        c1 = self.h5dict.get_dataset("chrms1")
        p1 = self.h5dict.get_dataset("cuts1")
        print("getting heatmap", chromosome, start, end)
        if end == None:
            end = self.genome.chrmLens[chromosome]
        low = h5dictBinarySearch(c1, p1, (chromosome, start), "left")
        high = h5dictBinarySearch(c1, p1, (chromosome, end), "right")
        c1 = self._getVector("chrms1", low, high)
        c2 = self._getVector("chrms2", low, high)
        p1 = self._getVector("cuts1", low, high)
        p2 = self._getVector("cuts2", low, high)
        mask = (c1 == c2) * (p2 >= start) * (p2 < end)
        p1 = p1[mask]
        p2 = p2[mask]
        del c1
        del c2
        assert start % resolution == 0

        heatmapSize = int(np.ceil((end - start) / float(resolution)))


        if len(p1) == 0:
            heatmap = np.zeros((heatmapSize, heatmapSize),
                           dtype="int32", order="C")
            return heatmap

        p1 -= start
        p2 -= start

        ind1 = p1 // resolution + heatmapSize * (p2 // resolution)
        inds = np.bincount(ind1, minlength=heatmapSize * heatmapSize)
        inds = np.array(inds, dtype = np.int32)
        inds.shape = ((heatmapSize, heatmapSize))
        heatmap = inds + inds.T


        if countDiagonalReads.lower() == "once":
            diag = np.diag(heatmap).copy()
            fillDiagonal(heatmap, diag / 2)
            del diag
        elif countDiagonalReads.lower() == "twice":
            pass
        else:
            raise ValueError("Bad value for countDiagonalReads")

        return heatmap

    def setSimpleHighResHeatmap(self):
        self.simpleHeatmap = True


    def getHiResHeatmapWithOverlaps(self, resolution, chromosome, start=0, end=None, countDiagonalReads="Twice", maxBinSpawn=10):
        if hasattr(self, "simpleHeatmap"):
            return self.getHiResHeatmap(resolution, chromosome, start=0, end=None, countDiagonalReads="Twice", maxBinSpawn=10)

        c1 = self.h5dict.get_dataset("chrms1")
        p1 = self.h5dict.get_dataset("cuts1")
        print("getting heatmap", chromosome, start, end)
        from scipy import weave
        if end == None:
            end = self.genome.chrmLens[chromosome]
        low = h5dictBinarySearch(c1, p1, (chromosome, start), "left")
        high = h5dictBinarySearch(c1, p1, (chromosome, end), "right")


        c1 = self._getVector("chrms1", low, high)
        c2 = self._getVector("chrms2", low, high)
        mids1 = self._getVector("mids1", low, high)
        mids2 = self._getVector("mids2", low, high)
        fraglens1 = self._getVector("fraglens1", low, high)
        fraglens2 = self._getVector("fraglens2", low, high)

        mask = (c1 == c2) * (mids2 >= start) * (mids2 < end)
        mids1 = mids1[mask]
        mids2 = mids2[mask]
        fraglens1 = fraglens1[mask]
        fraglens2 = fraglens2[mask]

        low1 = mids1 - fraglens1 / 2 - start
        high1 = low1 + fraglens1
        low2 = mids2 - fraglens2 / 2 - start
        high2 = low2 + fraglens2

        low1 = low1 / float(resolution)
        high1 = high1 / float(resolution)
        low2 = low2 / float(resolution)
        high2 = high2 / float(resolution)

        N = len(low1)  # @UnusedVariable

        if chromosome == 1:
            pass
            # 0/0

        heatmapSize = int(np.ceil((end - start) / float(resolution)))

        heatmap = np.zeros((heatmapSize, heatmapSize),
                           dtype="float64", order="C")




        code = r"""
            #line 1045 "fragmentHiC.py"
            double vector1[1000];
            double vector2[1000];

            for (int readNum = 0;  readNum < N; readNum++)
            {
                for (int i=0; i<10; i++)
                {
                    vector1[i] = 0;
                    vector2[i] = 0;
                }

                double l1 = low1[readNum];
                double l2 = low2[readNum];
                double h1 = high1[readNum];
                double h2 = high2[readNum];


                if ((h1 - l1) > maxBinSpawn) continue;
                if ((h2 - l2) > maxBinSpawn) continue;

                int binNum1 = ceil(h1) - floor(l1);
                int binNum2 = ceil(h2) - floor(l2);
                double binLen1 = h1 - l1;
                double binLen2 = h2 - l2;

                int b1 = floor(l1);
                int b2 = floor(l2);

                if (binNum1 == 1)
                    vector1[0] = 1.;
                else
                    {
                    vector1[0] = (ceil(l1+ 0.00000001) - l1) / binLen1;
                    for (int t = 1; t< binNum1 - 1; t++)
                        {vector1[t] = 1. / binLen1;}
                    vector1[binNum1 - 1] = (h1 - floor(h1)) / binLen1;
                    }

                if (binNum2 == 1) vector2[0] = 1.;

                else
                    {
                    vector2[0] = (ceil(l2 + 0.00000001) - l2) / binLen2;
                    for (int t = 1; t< binNum2 - 1; t++)
                        {vector2[t] = 1. / binLen2;}
                    vector2[binNum2 - 1] = (h2 - floor(h2)) / binLen2;
                    }
                if ((b1 + binNum1) >= heatmapSize) { continue;}
                if ((b2 + binNum2) >= heatmapSize) { continue;}
                if ((b1 < 0)) {continue;}
                if ((b2 < 0)) {continue;}
                double psum = 0;
                for (int i = 0; i< binNum1; i++)
                    {
                    for (int j = 0; j < binNum2; j++)
                        {
                        heatmap[(b1 + i) * heatmapSize +  b2 + j] += vector1[i] * vector2[j];
                        psum += vector1[i] * vector2[j];
                        }
                    }
                    if (abs(psum-1) > 0.0000001)
                        {
                            printf("bins num 1 = %d \n",binNum1);
                            printf("bins num 2 = %d \n",binNum2);
                            printf("psum = %f \n", psum);
                        }

                }
        """
        weave.inline(code,
                         ['low1', "high1", "low2", "high2",
                           "N", "heatmap", "maxBinSpawn",
                          "heatmapSize",
                           ],
                         extra_compile_args=['-march=native  -O3 '],
                         support_code=r"""
                        #include <stdio.h>
                        #include <math.h>
                        """)

        for i in range(len(heatmap)):
            heatmap[i, i:] += heatmap[i:, i]
            heatmap[i:, i] = heatmap[i, i:]
        if countDiagonalReads.lower() == "once":
            diag = np.diag(heatmap).copy()
            fillDiagonal(heatmap, diag / 2)
            del diag
        elif countDiagonalReads.lower() == "twice":
            pass
        else:
            raise ValueError("Bad value for countDiagonalReads")
        weave.inline("")  # to release all buffers of weave.inline
        gc.collect()
        return heatmap


    def saveHiResHeatmapWithOverlaps(self, filename, resolution, countDiagonalReads="Twice", maxBinSpawn=10, chromosomes="all"):
        """Creates within-chromosome heatmaps at very high resolution,
        assigning each fragment to all the bins it overlaps with,
        proportional to the area of overlaps.

        Parameters
        ----------
        resolution : int or str
            Resolution of a heatmap.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        maxBinSpawn : int, optional, not more than 10
            Discard read if it spawns more than maxBinSpawn bins

        """

        if not self._isSorted():
            print("Data is not sorted!!!")
            self._sortData()
        tosave = mirnylib.h5dict.h5dict(filename)
        if chromosomes == "all":
            chromosomes = list(range(self.genome.chrmCount))
        for chrom in chromosomes:
            heatmap = self.getHiResHeatmapWithOverlaps(resolution, chrom,
                               countDiagonalReads=countDiagonalReads, maxBinSpawn=maxBinSpawn)
            tosave["{0} {0}".format(chrom)] = heatmap
            del heatmap
            gc.collect()
        print("----> By chromosome Heatmap saved to '{0}' at {1} resolution".format(filename, resolution))

    def saveSuperHighResMapWithOverlaps(self, filename, resolution, chunkSize=20000000, chunkStep=10000000, countDiagonalReads="Twice", maxBinSpawn=10, chromosomes="all"):
        """Creates within-chromosome heatmaps at very high resolution,
        assigning each fragment to all the bins it overlaps with,
        proportional to the area of overlaps.

        Parameters
        ----------
        resolution : int or str
            Resolution of a heatmap.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        maxBinSpawn : int, optional, not more than 10
            Discard read if it spawns more than maxBinSpawn bins

        """

        tosave = mirnylib.h5dict.h5dict(filename)
        if chromosomes == "all":
            chromosomes = list(range(self.genome.chrmCount))
        for chrom in chromosomes:
            chrLen = self.genome.chrmLens[chrom]
            chunks = [(i * chunkStep, min(i * chunkStep + chunkSize, chrLen)) for i in range(chrLen // chunkStep + 1)]
            for chunk in chunks:
                heatmap = self.getHiResHeatmapWithOverlaps(resolution, chrom,
                                    start=chunk[0], end=chunk[1],
                                   countDiagonalReads=countDiagonalReads, maxBinSpawn=maxBinSpawn)
                tosave["{0}_{1}_{2}".format(chrom, chunk[0], chunk[1])] = heatmap
        print("----> Super-high-resolution heatmap saved to '{0}' at {1} resolution".format(filename, resolution))


    def exitProgram(self, a):
        print(a)
        print("     ----> Bye! :) <----")
        exit()


    def iterativeCorrection(self, numsteps=10, normToLen=False):
        '''
        Perform fragment-based iterative correction of Hi-C data.
        '''

        rfragLensConc = np.concatenate(self.genome.rfragLens)
        weights = np.ones(self.N, dtype=np.float32)
        concRfragAbsIdxs = np.r_[self.rfragAbsIdxs1, self.rfragAbsIdxs2]
        concOrigArgs = np.r_[np.arange(0, self.N), np.arange(0, self.N)]
        concArgs = np.argsort(concRfragAbsIdxs)
        concRfragAbsIdxs = concRfragAbsIdxs[concArgs]
        concOrigArgs = concOrigArgs[concArgs]
        fragBorders = np.where(concRfragAbsIdxs[:-1] != concRfragAbsIdxs[1:])[0] + 1
        fragBorders = np.r_[0, fragBorders, 2 * self.N]
        rfragLensLocal = rfragLensConc[concRfragAbsIdxs[fragBorders[:-1]]]
        for _ in range(numsteps):
            for i in range(len(fragBorders) - 1):
                mask = concOrigArgs[fragBorders[i]:fragBorders[i + 1]]
                totWeight = weights[mask].sum()
                if normToLen:
                    weights[mask] *= rfragLensLocal[i] / totWeight
                else:
                    weights[mask] /= totWeight

        self.vectors['weights'] = 'float32'
        self.weights = weights


    def plotScaling(self, fragids1=None, fragids2=None,
                    # IDs of fragments for which to plot scaling.
                    # One can, for example, limit oneself to
                    # only fragments shorter than 1000 bp
                    # Or calculate scaling only between different arms

                    useWeights=False,
                        # use weights associated with fragment length
                    excludeNeighbors=None, enzyme=None,
                        # number of neighboring fragments to exclude.
                        # Enzyme is needed for that!
                    normalize=True, normRange=None,
                        # normalize the final plot to sum to one
                    withinArms=True,
                        # Treat chromosomal arms separately
                    mindist=1000,
                        # Scaling was proved to be unreliable
                        # under 10000 bp for 6-cutter enzymes
                    maxdist=None,

                    #----Calculating scaling within a set of regions only----
                    regions=None,
                    # Array of tuples (chrom, start, end)
                    # for which scaling should be calculated
                    # Note that calculation might be extremely long
                    # (it might be proportional to # of regions for # > 100)

                    appendReadCount=True,
                    dirs = ['unidir'],
                    **kwargs
                        # Append read count to the plot label
                        # kwargs to be passed to plotting
                    ):  # Sad smiley, because this method
                        # is very painful and complicated
        """plots scaling over, possibly uses subset of fragmetns, or weigts,
        possibly normalizes after plotting

        Plan of scaling calculation:

        1. Subdivide all genome into regions. \n
            a. Different chromosomes \n
            b. Different arms \n
            c. User defined squares/rectangles on a contact map \n
               -(chromosome, start,end) square around the diagonal \n
               -(chr, st1, end1, st2, end2) rectangle \n

        2. Use either all fragments, or only interactions between
        two groups of fragments \n
            e.g. you can calculate how scaling for small fragments is different
            from that for large \n
            It can be possibly used for testing Hi-C protocol issues. \n
            One can see effect of weights by doing this \n

        3. (optional) Calculate correction associated
        with fragment length dependence

        4. Subdivide all possible genomic separation into log-spaced bins

        5. Calculate expected number of fragment pairs within each bin
        (possibly with weights from step 3).

        If exclusion of neighbors is specificed,
        expected number of fragments knows about this

        Parameters
        ----------
        fragids1, fragids2 : np.array of fragment IDs, optional
            Scaling is calculated only for interactions between
            fragids1 and fragids2
            If omitted, all fragments are used
            If boolean array is supplied, it serves as a mask for fragments.
        useWeights : bool, optional
            Use weights calculated from fragment length
        excludeNeighbors : int or None, optional
            If None, all fragment pairs are considered.
            If integer, only fragment pairs separated
            by at least this number of r-fragments are considered.
        enzyme : string ("HindIII","NcoI")
            If excludeNeighbors is used, you have to specify restriction enzyme
        normalize : bool, optional
            Do an overall normalization of the answer, by default True.
        withinArms : bool, optional
            Set to false to use whole chromosomes instead of arms
        mindist, maxdist : int, optional
            Use lengthes from mindist to maxdist
        regions : list of (chrom,start,end) or (ch,st1,end1,st2,end2), optional
            Restrict scaling calculation to only certain squares of the map
        appendReadCount : bool, optional
            Append read count to the plot label
        plot : bool, optional
            If False then do not display the plot. True by default.
        dirs : list, optional.
            The list of fragment pair directionalities used to calculate the scaling.
            The possible values are:
            -- 'convergent' - fragment pairs that face towards each other
            -- 'divergent' - fragment pairs that face away from each other
            -- 'unidir' - fragment pairs oriented into the same direction
            The default value is ['unidir'].
        **kwargs :  optional
            All other keyword args are passed to plt.plot

        Returns
        -------
        (bins,probabilities) - values to plot on the scaling plot

        """
        # TODO:(MI) write an ab-initio test for scaling calculation
        if not self._isSorted():
            self._sortData()
        import matplotlib.pyplot as plt
        if (excludeNeighbors is None) or (excludeNeighbors < 0):
            excludeNeighbors = 0

        # use all fragments if they're not specified
        # parse fragment array if it's bool
        if (fragids1 is None) and (fragids2 is None):
            allFragments = True
        else:
            allFragments = False

        if fragids1 is None:
            fs = self.fragmentSum()
            fragids1 = fs > 0
        if fragids2 is None:
            try:
                fragids2 = fs > 0
            except:
                fragids2 = self.fragmentSum() > 0
        if "fs" in locals():
            del fs

        if fragids1.dtype == np.bool:
            fragids1 = self.rFragIDs[fragids1]
        if fragids2.dtype == np.bool:
            fragids2 = self.rFragIDs[fragids2]

        # Calculate regions if not specified


        if regions is None:
            if withinArms == False:
                regions = [(i, 0, self.genome.chrmLens[i])
                    for i in range(self.genome.chrmCount)]
            else:
                regions = [(i, 0, self.genome.cntrMids[i])
                    for i in range(self.genome.chrmCount)] + \
                    [(i, self.genome.cntrMids[i], self.genome.chrmLens[i])
                    for i in range(self.genome.chrmCount)]

        if maxdist is None:
            maxdist = max(
                        max([i[2] - i[1] for i in regions]),
                        # rectangular regions
                        max([abs(i[2] - i[3]) for i in regions if
                             len(i) > 3] + [0]),
                        max([abs(i[1] - i[4]) for i in regions if
                             len(i) > 3] + [0])  # other side
                          )
        # Region to which a read belongs

        fragch1 = fragids1 // self.fragIDmult
        fragch2 = fragids2 // self.fragIDmult
        fragpos1 = fragids1 % self.fragIDmult
        fragpos2 = fragids2 % self.fragIDmult

        c1_h5 = self.h5dict.get_dataset("chrms1")
        p1_h5 = self.h5dict.get_dataset("cuts1")
        c2_h5 = self.h5dict.get_dataset("chrms2")
        p2_h5 = self.h5dict.get_dataset("cuts2")

        bins = np.array(
            numutils.logbins(mindist, maxdist, 1.12), float) + 0.1  # bins of lengths
        numBins = len(bins) - 1  # number of bins

        args = np.argsort(self.rFragIDs)
        usort = self.rFragIDs[args]

        if useWeights == True:  # calculating weights if needed
            try:
                self.fragmentWeights
            except:
                self.calculateFragmentWeights()
            uweights = self.fragmentWeights[args]  # weights for sorted fragment IDs
            weights1 = uweights[np.searchsorted(usort, fragids1)]
            weights2 = uweights[np.searchsorted(usort, fragids2)
                ]  # weghts for fragment IDs under  consideration




        observed = [0] * (len(bins) - 1)
        expected = [0] * (len(bins) - 1)

        binBegs, binEnds = bins[:-1], bins[1:]
        binMids = 0.5 * (binBegs + binEnds).astype(float)
        binLens = binEnds - binBegs


        for  region in regions:
            numExpFrags = np.zeros(numBins)  # count of reads in each min
            if len(region) == 3:
                chrom, start1, end1 = region
                low = h5dictBinarySearch(c1_h5, p1_h5, (chrom, start1), "left")
                high = h5dictBinarySearch(c1_h5, p1_h5, (chrom, end1), "right")
            if len(region) == 5:
                chrom, start1, end1, start2, end2 = region
                assert start1 < end1
                assert start2 < end2
                low = h5dictBinarySearch(c1_h5, p1_h5, (chrom, min(start1, start2)), "left")
                high = h5dictBinarySearch(c1_h5, p1_h5, (chrom, max(end1, end2)), "right")
            chr2 = c2_h5[low:high]
            pos1 = p1_h5[low:high]
            pos2 = p2_h5[low:high]

            myfragids1 = self._getVector("fragids1", low, high)
            myfragids2 = self._getVector("fragids2", low, high)
            mystrands1 = self._getVector("strands1", low, high)
            mystrands2 = self._getVector("strands2", low, high)
            mydists = self._getVector("distances", low, high)
            print("region", region, "low", low, "high", high)

            if len(region) == 3:
                mask = (pos1 > start1) * (pos1 < end1) * \
                (chr2 == chrom) * (pos2 > start1) * (pos2 < end1)

                maskFrag1 = (fragch1 == chrom) * (fragpos1 >
                    start1) * (fragpos1 < end1)
                maskFrag2 = (fragch2 == chrom) * (fragpos2 >
                    start1) * (fragpos2 < end1)

            if len(region) == 5:
                chrom, start1, end1, start2, end2 = region

                mask1 = (chr2 == chrom) * (pos1 > start1) * \
                (pos1 < end1) * (pos2 > start2) * (pos2 < end2)

                mask2 = (chr2 == chrom) * (pos1 > start2) * \
                (pos1 < end2) * (pos2 > start1) * (pos2 < end1)
                mask = mask1 + mask2

                maskFrag1 = (fragch1 == chrom) * (
                    (fragpos1 > start1) * (fragpos1 < end1)
                    )  # + (fragpos1 > start2) * (fragpos1 < end2))
                maskFrag2 = (fragch2 == chrom) * (
                    (fragpos2 > start2) * (fragpos2 < end2)
                    )  # + (fragpos2 > start1) * (fragpos2 < end1))

            if maskFrag1.sum() == 0 or maskFrag2.sum() == 0:
                print("no fragments for region", region)
                continue

            if mask.sum() == 0:
                print("No reads for region", region)
                continue
            chr2 = chr2[mask]
            pos1 = pos1[mask]
            pos2 = pos2[mask]
            myfragids1 = myfragids1[mask]
            myfragids2 = myfragids2[mask]
            mystrands1 = mystrands1[mask]
            mystrands2 = mystrands2[mask]
            mydists = mydists[mask]

            validFragPairs = np.ones(len(chr2), dtype=np.bool)
            if allFragments == False:
                # Filter the dataset so it has only the specified fragments.
                p11 = arrayInArray(myfragids1, fragids1)
                p12 = arrayInArray(myfragids1, fragids2)
                p21 = arrayInArray(myfragids2, fragids1)
                p22 = arrayInArray(myfragids2, fragids2)
                validFragPairs *= ((p11 * p22) + (p12 * p21))

        # Consider pairs of fragments from the same region.

        # Keep only --> -->  or <-- <-- pairs, discard --> <-- and <-- -->


            dirMask = np.zeros_like(mystrands1)
            for directionality in dirs:
                if directionality == 'unidir':
                    dirMask += (mystrands1 == mystrands2)
                elif directionality == 'convergent':
                    dirMask += (pos1<pos2) * (mystrands1) * (~mystrands2)
                    dirMask += (pos1>pos2) * (~mystrands1) * (mystrands2)
                elif directionality == 'divergent':
                    dirMask += (pos1<pos2) * (~mystrands1) * (mystrands2)
                    dirMask += (pos1>pos2) * (mystrands1) * (~mystrands2)
                else:
                    raise Exception(
                        'Unknown fragment pair directionality: {}'.format(directionality))


            print(dirMask.sum())
            validFragPairs *= dirMask

        # Keep only fragment pairs more than excludeNeighbors fragments apart.
            distsInFrags = self.genome.getFragmentDistance(
                myfragids1, myfragids2, self.genome.enzymeName)

            validFragPairs *= (distsInFrags > excludeNeighbors)

            print(validFragPairs.sum())
            distances = np.sort(mydists[validFragPairs])

            "calculating fragments lengths for exclusions to expected # of counts"
            # sorted fragment IDs and lengthes


            print(region)

            # filtering fragments that correspond to current region
            bp1, bp2 = fragpos1[maskFrag1], fragpos2[maskFrag2]
                # positions of fragments on chromosome

            p2arg = np.argsort(bp2)
            p2 = bp2[p2arg]  # sorted positions on the second fragment

            if excludeNeighbors != 0:
                "calculating excluded fragments (neighbors) and their weights"\
                " to subtract them later"
                excFrag1, excFrag2 = self.genome.getPairsLessThanDistance(
                    fragids1[maskFrag1], fragids2[maskFrag2], excludeNeighbors, enzyme)
                excDists = np.abs(excFrag2 - excFrag1)
                    # distances between excluded fragment pairs
                if useWeights == True:
                    correctionWeights = weights1[numutils.arraySearch(
                        fragids1, excFrag1)]

                    # weights for excluded fragment pairs
                    correctionWeights = correctionWeights * weights2[
                                numutils.arraySearch(fragids2, excFrag2)]
            if useWeights == True:
                w1, w2 = weights1[mask1], weights2[mask2]
                sw2 = np.r_[0, np.cumsum(w2[p2arg])]
                    # cumsum for sorted weights on 2 strand

            for minDist, maxDist, binIndex in zip(binBegs, binEnds, list(range(numBins))):
                "Now calculating actual number of fragment pairs for a "\
                "length-bin, or weight of all these pairs"

                # For each first fragment in a pair, calculate total # of
                # restriction fragments in the genome lying downstream within
                # the bin.
                val1 = np.searchsorted(p2, bp1 - maxDist)
                val2 = np.searchsorted(p2, bp1 - minDist)
                if useWeights == False:
                    curcount = np.sum(np.abs(val1 - val2))  # just # of fragments
                else:
                    # (difference in cumsum of weights) * my weight
                    curcount = np.sum(w1 * np.abs(sw2[val1] - sw2[val2]))

                # Repeat the procedure for the fragments lying upstream.
                val1 = np.searchsorted(p2, bp1 + maxDist)
                val2 = np.searchsorted(p2, bp1 + minDist)
                if useWeights == False:
                    curcount += np.sum(np.abs(val1 - val2))
                else:
                    curcount += np.sum(w1 * np.abs(sw2[val1] - sw2[val2]))

                # now modifying expected count because of excluded fragments
                if excludeNeighbors != 0:
                    if useWeights == False:
                        ignore = ((excDists > minDist) *
                            (excDists < maxDist)).sum()
                    else:
                        ignore = (correctionWeights[((excDists > minDist) * \
                                                (excDists < maxDist))]).sum()

                    if (ignore >= curcount) and (ignore != 0):
                        if ignore < curcount * 1.0001:
                            curcount = ignore = 0
                        else:
                            print("error found", "minDist:", minDist)
                            print("  curcount:", curcount, "  ignore:", ignore)
                    else:  # Everything is all right
                        curcount -= ignore
                numExpFrags[binIndex] += curcount
                # print curcount


            for i in range(len(bins) - 1):  # Dividing observed by expected
                first, last = tuple(np.searchsorted(distances, [binBegs[i], binEnds[i]]))
                mycounts = last - first
                # print mycounts, numExpFrags[i]
                observed[i] += (mycounts)
                expected[i] += (numExpFrags[i])
                # print i, mycounts, numExpFrags[i]
            # print "values", values
            # print "rawValies", rawValues

        totalSum = np.sum(observed)
        values = np.array(observed) / np.array(expected)
        if np.sum(observed) == 0:
            return None

        if normalize == True:
            if normRange is None:
                values /= np.sum(
                    1. * (binLens * values)[
                        np.logical_not(
                            np.isnan(binMids * values))])
            else:
                values /= np.sum(
                    1. * (binLens * values)[
                        np.logical_not(
                            np.isnan(binMids * values))
                            * (binMids > normRange[0])
                            * (binMids < normRange[1])])

        do_plot = kwargs.pop('plot', True)
        if do_plot:
            if appendReadCount == True:
                if "label" in list(kwargs.keys()):
                    kwargs["label"] = "{0}, {1} reads".format(kwargs["label"], totalSum)
            plt.plot(binMids, values, **kwargs)
        return (binMids, values)

    def plotRsiteStartDistribution(self, offset=5, length=200):
        """
        run plt.show() after this function.
        """
        import matplotlib.pyplot as plt
        dists1 = self.fraglens1 - np.array(self.dists1, dtype="int32")
        dists2 = self.fraglens2 - np.array(self.dists2, dtype="int32")
        m = min(dists1.min(), dists2.min())
        if offset < -m:
            offset = -m
            print("minimum negative distance is %d, larger than offset;"\
            " offset set to %d" % (m, -m))
        dists1 += offset
        dists2 += offset
        myrange = np.arange(-offset, length - offset)

        plt.subplot(141)
        plt.title("strands1, side 1")
        plt.plot(myrange, np.bincount(
            5 + dists1[self.strands1 == True])[:length])
        plt.subplot(142)
        plt.title("strands1, side 2")
        plt.plot(myrange, np.bincount(
            dists2[self.strands1 == True])[:length])

        plt.subplot(143)
        plt.title("strands2, side 1")
        plt.plot(myrange, np.bincount(
            dists1[self.strands1 == False])[:length])
        plt.subplot(144)
        plt.title("strands2, side 2")
        plt.plot(myrange, np.bincount(
            dists2[self.strands1 == False])[:length])
